{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 31 08:18:36 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 40%   49C    P8    31W / 260W |      1MiB / 11016MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:81:00.0 Off |                  N/A |\n",
      "| 56%   77C    P2   258W / 260W |   2136MiB / 11019MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A     27680      C   python                           2133MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### Install `qqdm`\n",
    "# Check if installed\n",
    "try:\n",
    "    import qqdm\n",
    "except:\n",
    "    ! pip install qqdm > /dev/null 2>&1\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_dir = '.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABEUlEQVR4nGP8z4AbMOGRQ5F8uekLmux/BNjHs+0/CkDWqcKJppMFQv1jYGBgYGb69w/FMsb/DAwMDD+bnjMwMHxbb6nIyMDAwMBWqoyk8/+zRwwMDD//vGFmYGBgYBDhQNbJ8Pvvp7t/OL0nBENMZUG1s2vFsz+C71nYsPnz5QSz/Vt1f//DGgj/GV0MbEMYTqDJQrz7Sc/62VVZZtefKIEAlfy3XthM3TBD7Qu2EGL07ZPWncHGzog9bP/+/v1EvvUv9rBlYvk37VsgE1ad/34+zeeL+4UaKywMDAwM7w7/unH46puSKlZUjYz/GRj+z278y2xkbW7Cy4ApyfD1838mQVY0lzLAAx47IDqBDQpJAN4Euv7fFejQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F9C0A3760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABNklEQVR4nGP8zwAH//8xM6AAJiT2pdTXuCVfbvmGW5LhPwNuSUaGP7gl5ZkuoUqyMDAwMLw78I9PjVVYSu2AP9P//39ZUSQfFP/8/4dVWvER9y6GC08+T+dCltQ9zvD5wZPr9/7uPsMkzuXKgnAhHPz71aJ07eHHH3/gIghVDIwsEv9ERHF6RevDG9yBwMn8GZvk/2+3nvxnEOe4g+rR////////scmBX+nov+OCV/4jA4jkNR73ed4aD3M032GRvME/5ddV4QKRqX+xSH4vFM4/4cmg+eQ/Fsn/X6doiPDy7vmHVfL/3xdzjB2+o8r9h/mTSTxBn4UJ1SNIgfD1iTS6JDzgfxSJ7UEzFWbnr5fZQrP+YJf8FKcpsegHuhw0yti02QI9MGxkYPwPtZmREUMOJokdAAB60yoWf/hgewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F95141FD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABN0lEQVR4nGP8z4AbsCBz/j9lkGZE4jMi6/wTz7AQWTWKToafqMYy4bESXfI/bklG8Yc/cEoyGbz59R8OIA76eBNq2v/7P/c8gJnM6KHHwsDAcC7iN8y133KEYR7lMmdg/M/A8PUxTPWFtCXOMElGTkYWBgYGbg24rawSPEhuQATC/6dPGe7/Q/EKQvJa0GuGH39RPQpz+FNL0zNXW9gyv/1H8gyU/tuicOXf/6tcEs+RJGGB8HOnrwYjA1r4wSQ/3tP5/vXLpV+i7EiSsPh85/JEmJHh/eelfoyYkv8f7nvNwHDy7HkhbK79/+/fv19JFl/+Y3EQAwMjI+PrA36cWP35////m55y15E1/keSfOurdvAvdskfm/3kdv37j1Xy33Jh7ZWo+v7/h6fbV49UedGTIiO+7AAAZ4kCU7KEzEEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F9C0A3670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABLUlEQVR4nGP8z4AbMKHx/3/7g1vyredeBIcFTfLP7c8w5r+/EMlX90UEISLv/315B5VbepDxPwMDA8OiAmYuiNDfZ0LcUNs/BkEkPzw4D3XHpyZHL0YIU9ydEeYVKP3dW3g51B2MCNcyQgCn47VfUCamVxjY/uH2JwpAlUQLS+RAeH3rnLbIN+ySn6JPsP35/18Kq7Hvz/edPhr75f9/bDoZGLgUGJveb32hgkUnn0H3ob/8vJ8PILT+R4BLMqIXDssyhPyGCTAiuf7fwQiuj9o/5FbC7EK2k8l+2RnFe20WjNiM/f///7+frp6v4Tz04HvyUlaIAbvOf1eNJQ8juKiSXy2lt/7FIfl7Dl/rn//YJf+tFrR7jKwY2Z8MZw/5qDAi8VEkGf4jSzEwAABSseqGZyInRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F95141F40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABJklEQVR4nGP8z4AbMKFy/+GRfF54D7fkh8Wv8RgLAxCXsGCR+Pfx0BtTfZjkz5dQNz/79+Lhv32HTj4TbtFjZGBg/M/AwHDO/xdE8s87Qdb/v3WsnYz5WBmgkh9PQ73wqLBVg0FAhwPmkv/I4JrgCWQukoN+/2NECy645P9bTXf4PVFDCG7sU33JNFMOwZvIxsIk/03k2/PnvpTsW2RJeAh9lzRjFtdBNRUuyfjzCwOHP2powhzEaPBypvvlvWjOhZn/xlZYUIgH1U641/5/e/6bufT8BSFs/mTkVmH4+gKHgxgYGBh+PmdhxCn5/a85D1YH/f///0Mk336UeEBI/ntXzdv9C7vkvyOWvJnf/2OX/GxlMBNNDuHPvxdlRFGcygBNJrgAAEPeDmCQZ6aqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F95158040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABOElEQVR4nGP8z4AbMKHxXz1AVv0fBfxJt/+B4KHp/P/j7U+cxrKEvHyD2041ppP/cUrKqh34D3EKAwMDC5okq9Sdrz9/3v50ysqTEUny//9/v169/K+01e/Rjz8MUppQnf8YGBi+3bxw+fGLu78Y/nxlCjHXZRfiYmRgYPzP8KfzPsO/iw+4FUUEHAyZX0R0xcIcwsLA8P/lQwYmTzsDXlbGX6f/fGf8zYgcQr9//fr19/////9fRgsKczLO/occQiysrKxMDAwM/5fumr0vmEUcrhPZK38OmvowCkiaYQ0EJoWbuyvmWggjRJDj5IkrPw9D8V84Hyb5bsO3////v9sQJHbxP4bkWYmajXcfnM4QWPQHU/JXjbSYqKiwRj9SXP9nhEXQz/cfH/3n0BJCdiEjKQlscEsCAN5i3onYmdekAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F95141DF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABO0lEQVR4nGP8z4AbMOGRo5bks4v/UCRZkNj/ly06zs3w4+eLf4yK7OiSDH9+/Xl+ZObDNwxcG00Qkk+v/WdgYPh/+1XWic9ySSa87DpIxu4v/sfAwMDw69OtpFAxfkaYSYz/GRgYGL68ZWBgYGA4H39IDy4D18nDw8DAwMDwnIkNWY6CQIB55T+2CIBI/j1+8B4DA5P0P2ySF4NYVVkYPi1m/IEq+///////Fwge+/Hz5/sipvif/5EARPKW2Mx/////P8wi+w5ZEmKsrF4vhx3jh/6/aO6CqLkfxS0iIiQbJvsWWScjVOnnM78ZWNTW95wXwuJPXkcGBoY/x5S5cIbQlYOubJh2/v318+fPn7ctLd78x3Dt//Uz/zEwMDxmXymMGUKMCop/GBgY1BI0UAMI6tp/mA5ASGIHADm3qpNJq4xdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F9C7A20A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABI0lEQVR4nGP8z4AbMKFy7yz/jsz9jwz+5su//P///79///7+/v//Pwuqzi9KfP9/Xdv1juEXZxMLA5Lk/39/3r153Mdw9BS/CANjNBMDAyPEQf8ffv92/vizq8+4pBlVg12FGBhYmeB2/nARFJSxTpokWfXpy89/MCdAjWWd8IVRjo/9+3Q+HkaERVBJJm0Ghp9ff6B5GuGg36WbGNXe4AiEd5tMC869xqHzwPcq7XuTsev8/5lTnlWPEUUSKRB+3+L/z4VdklH8qyfj/x84dHrs/8XwOxGHJKshA8NXjof/mLF5hYGBgYFD/84/bK5lYPj///+PxziMfbTg6/+nZ9KYsEo+2PWHgasoE8lKWHwyMDD8/8XAwMiKEgqMJKS+gZcEAF56gf6wykc6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F9C0A37C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABXklEQVR4nGP8z4AbMOGRI1Xy9z/ckp+TVv3HKfnn6Lq/MDYLuiSHMkTjv98MzFDJv/9ZGBj+//339ef1symvmN49/bnn1H9rxv8MDAwM/yecjhFmuLPl7bMPv98IszH8ZmCUtuW0h+rkvhT3n4FVV95M/1tpgBsDpxazIDcjIyPUhq/P/zJwSLIxMr4z9u1nRnUQEy8vAwMDw3+G/yw8H+GOQ3bt/w9bnrOZfTZjwiL5/0LaPd63PJ/YsYXQ6wSezacOW/6+jYio/zDwb7Hkuf///5/ij/sDE0Lo/H9IXYOB4deET9+whS3vrbmvfx3axmTJjGns/6dJMpp+4gxid+EiSJL/f98sc7Ph6PmDVfL//++zZWM+/8cu+TZfOOrdf2yS/15u8BCpe/8fm+TXfm1Bj4O//2OT/Dtf0O/g1///sUreMF707T86gEp+S1/2B0PuPzSZbPscgpHUGBgAt9BS1wiwXusAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F951580A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABQElEQVR4nGP8z4AbMKHwfvxA4TIi6/xbyNDPjMRnQVb5/wkDii2oxuK1Ew2gGMvwn5EBZjAjTPL3W4jAp6uGT86f+MfAwMDAnKwKde25oF8MDAwMDP9e8nD/kWNhYGBg4JymDZV8d+AvAwMDA8OX0mBXDSWIZ9gYGRgY/kPBv68Hdl6SmfXvPxKAOuj/g92rzzEwfcDmlf97nKs4Vu3y/IPml////////9VI5fL3//+v8KMaC9HJbs8kysHAIMT9D4vO/9eEJ/z59y6HMfEPFgcp+7XfF9tyjfMDSsBDJdl6eXf+F1s1GdVUeHz++cnAxOAlvAI5sOGxwsLNzfn9njlyXKNF2X8BLIGAAyBZ8f/zge9oamF++nG/S59L9RayN//DJP8tlpTL3XwbJfT+w73y7KShLDOqoajpFh3gdS0Aq5C/ToYG3GgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0F9C0A3760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "for i in range(10, 20):\n",
    "    im = Image.open(\"Omniglot/images_background/Japanese_(hiragana).0/character13/0500_\" + str (i) + \".png\")\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules we need\n",
    "import glob, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "try:\n",
    "    from qqdm.notebook import qqdm as tqdm\n",
    "except ModuleNotFoundError:\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import matplotlib.patches as patches\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# fix random seeds\n",
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(in_ch: int, out_ch: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "\n",
    "def ConvBlock4(in_ch: int, out_ch: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def ConvBlockFunction(x, w, b, w_bn, b_bn):\n",
    "    x = F.conv2d(x, w, b, padding=1)\n",
    "    x = F.batch_norm(x,\n",
    "                     running_mean=None,\n",
    "                     running_var=None,\n",
    "                     weight=w_bn, bias=b_bn,\n",
    "                     training=True)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def ConvBlockFunction4(x, w, b, w_bn, b_bn):\n",
    "    x = F.conv2d(x, w, b, padding=1)\n",
    "    x = F.batch_norm(x,\n",
    "                     running_mean=None,\n",
    "                     running_var=None,\n",
    "                     weight=w_bn, bias=b_bn,\n",
    "                     training=True)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2, stride=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_label(n_way, k_shot):\n",
    "    return (torch.arange(n_way)\n",
    "                 .repeat_interleave(k_shot)\n",
    "                 .long())\n",
    "\n",
    "# Try to create labels for 5-way 2-shot setting\n",
    "create_label(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, val_label):\n",
    "    \"\"\" utility function for accuracy calculation \"\"\"\n",
    "    acc = np.asarray([(\n",
    "        torch.argmax(logits, -1).cpu().numpy() == val_label.cpu().numpy())]\n",
    "        ).mean() \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_true(logits, val_label):\n",
    "    \"\"\" utility function for accuracy calculation \"\"\"\n",
    "    acc = np.asarray([(\n",
    "        torch.argmax(logits, -1).cpu().numpy() == val_label.cpu().numpy())]\n",
    "        )\n",
    "    acc= acc[0]\n",
    "    res=[]\n",
    "    for x in acc:\n",
    "        res =res+[bool(x)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Omniglot(Dataset):\n",
    "    def __init__(self, data_dir, k_way, q_query):\n",
    "        self.file_list = [f for f in glob.glob(\n",
    "            data_dir + \"**/character*\", \n",
    "            recursive=True)]\n",
    "        self.transform = transforms.Compose(\n",
    "                            [transforms.ToTensor()])\n",
    "        self.n = k_way + q_query\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = np.arange(20)\n",
    "\n",
    "        # For random sampling the characters we want.\n",
    "        np.random.shuffle(sample) \n",
    "        img_path = self.file_list[idx]\n",
    "        img_list = [f for f in glob.glob(\n",
    "            img_path + \"**/*.png\", recursive=True)]\n",
    "        img_list.sort()\n",
    "        imgs = [self.transform(\n",
    "            Image.open(img_file).resize((28,28)).convert('RGB'))\n",
    "#             Image.open(img_file).resize((56,56),Image.ANTIALIAS).convert('RGB'))\n",
    "            for img_file in img_list]\n",
    "        # `k_way + q_query` examples for each character\n",
    "        imgs = torch.stack(imgs)[sample[:self.n]] \n",
    "        return imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_update_MAML(fast_weights, loss, inner_lr):\n",
    "    \"\"\" Inner Loop Update \"\"\"\n",
    "    grads = torch.autograd.grad(\n",
    "        loss, fast_weights.values(), create_graph=True)\n",
    "    # Perform SGD\n",
    "    fast_weights = OrderedDict(\n",
    "        (name, param - inner_lr * grad)\n",
    "        for ((name, param), grad) in zip(fast_weights.items(), grads))\n",
    "    return fast_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gradients_MAML(\n",
    "    special_grad: OrderedDict, fast_weights, model, len_data):\n",
    "    \"\"\" Actually do nothing (just backwards later) \"\"\"\n",
    "    return special_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_update_MAML(model, meta_batch_loss, grad_tensors):\n",
    "    \"\"\" Simply backwards \"\"\"\n",
    "    meta_batch_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "k_shot = 5\n",
    "q_query = 15\n",
    "inner_train_step = 1\n",
    "inner_lr = 0.4\n",
    "meta_lr = 0.001\n",
    "meta_batch_size = 32\n",
    "max_epoch = 30\n",
    "eval_batches = test_batches = 20\n",
    "train_data_path = './Omniglot/images_background/'\n",
    "test_data_path = './Omniglot/images_evaluation/'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_init(datasets, num_workers=4):\n",
    "    train_set, val_set, test_set = datasets\n",
    "    train_loader = DataLoader(train_set,\n",
    "                            # The \"batch_size\" here is not \\\n",
    "                            #    the meta batch size, but  \\\n",
    "                            #    how many different        \\\n",
    "                            #    characters in a task,     \\\n",
    "                            #    i.e. the \"n_way\" in       \\\n",
    "                            #    few-shot classification.\n",
    "                            batch_size=n_way,\n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True)\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=n_way,\n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True)\n",
    "    test_loader = DataLoader(test_set,\n",
    "                            batch_size=n_way,\n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True)\n",
    "    train_iter = iter(train_loader)\n",
    "    val_iter = iter(val_loader)\n",
    "    test_iter = iter(test_loader)\n",
    "    return (train_loader, val_loader, test_loader), \\\n",
    "           (train_iter, val_iter, test_iter)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    Omniglot(train_data_path, k_shot, q_query), [3200, 656])\n",
    "test_set = Omniglot(test_data_path, k_shot, q_query)\n",
    "\n",
    "(train_loader, val_loader, test_loader), \\\n",
    "(train_iter, val_iter, test_iter) = dataloader_init(\n",
    "                             (train_set, val_set, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_batch(meta_batch_size,\n",
    "                   k_shot, q_query, \n",
    "                   data_loader, iterator):\n",
    "    data = []\n",
    "    for _ in range(meta_batch_size):\n",
    "        try:\n",
    "            # a \"task_data\" tensor is representing \\\n",
    "            #     the data of a task, with size of \\\n",
    "            #     [n_way, k_shot+q_query, 1, 28, 28]\n",
    "            task_data = iterator.next()  \n",
    "        except StopIteration:\n",
    "            iterator = iter(data_loader)\n",
    "            task_data = iterator.next()\n",
    "        train_data = (task_data[:, :k_shot]\n",
    "                      .reshape(-1, 3, 28, 28))\n",
    "        val_data = (task_data[:, k_shot:]\n",
    "                    .reshape(-1, 3,28, 28))\n",
    "        task_data = torch.cat(\n",
    "            (train_data, val_data), 0)\n",
    "        data.append(task_data)\n",
    "    return torch.stack(data).to(device), iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_model=torch.load('best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newClassifier(nn.Module):\n",
    "    def __init__(self, in_ch, k_way):\n",
    "        super(newClassifier, self).__init__()\n",
    "        self.conv1 = ConvBlock(in_ch, 64)\n",
    "        self.conv2 = ConvBlock(64, 64)\n",
    "        self.conv3 = ConvBlock(64, 64)\n",
    "        self.conv4 = ConvBlock4(64, 64)\n",
    "#         self.add_on_layers = nn.Sequential(\n",
    "#             nn.Conv2d(64,128,kernel_size=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(128,128,kernel_size=1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "        prototype_shape = (20,64,1,1)\n",
    "        self.prototype_shape = prototype_shape\n",
    "        self.img_size = 28\n",
    "        self.num_prototypes = prototype_shape[0]\n",
    "        self.num_classes = k_way\n",
    "        self.epsilon = 1e-4\n",
    "        self.prototype_activation_function =  'log'\n",
    "        assert(self.num_prototypes % self.num_classes == 0)\n",
    "        self.prototype_class_identity = torch.zeros(self.num_prototypes,\n",
    "                                                    self.num_classes)\n",
    "        num_prototypes_per_class = self.num_prototypes // self.num_classes\n",
    "        for j in range(self.num_prototypes):\n",
    "            self.prototype_class_identity[j, j // num_prototypes_per_class] = 1\n",
    "            \n",
    "        self.prototype_vectors = nn.Parameter(torch.rand(self.prototype_shape),\n",
    "                                              requires_grad=True)\n",
    "        self.ones = nn.Parameter(torch.ones(self.prototype_shape),\n",
    "                                 requires_grad=False)\n",
    "        self.logits = nn.Linear(self.num_prototypes, k_way\n",
    "                                    ,bias=False\n",
    "                               ) # do not use bias    \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def set_last_layer_incorrect_connection(self, incorrect_strength):\n",
    "        '''\n",
    "        the incorrect strength will be actual strength if -0.5 then input -0.5\n",
    "        '''\n",
    "        positive_one_weights_locations = torch.t(self.prototype_class_identity)\n",
    "        negative_one_weights_locations = 1 - positive_one_weights_locations\n",
    "\n",
    "        correct_class_connection = 1\n",
    "        incorrect_class_connection = incorrect_strength\n",
    "        self.logits.weight.data.copy_(\n",
    "            correct_class_connection * positive_one_weights_locations\n",
    "            + incorrect_class_connection * negative_one_weights_locations)\n",
    "    \n",
    "    \n",
    "    def functional_forward(self, x, params):\n",
    "        '''\n",
    "        Arguments:\n",
    "        x: input images [batch, 1, 28, 28]\n",
    "        params: model parameters, \n",
    "                i.e. weights and biases of convolution\n",
    "                     and weights and biases of \n",
    "                                   batch normalization\n",
    "                type is an OrderedDict\n",
    "\n",
    "        Arguments:\n",
    "        x: input images [batch, 1, 28, 28]\n",
    "        params: The model parameters, \n",
    "                i.e. weights and biases of convolution \n",
    "                     and batch normalization layers\n",
    "                It's an `OrderedDict`\n",
    "        '''\n",
    "        for block in [1, 2,3]:\n",
    "            x = ConvBlockFunction(\n",
    "                x,\n",
    "                params[f'conv{block}.0.weight'],\n",
    "                params[f'conv{block}.0.bias'],\n",
    "                params.get(f'conv{block}.1.weight'),\n",
    "                params.get(f'conv{block}.1.bias'))\n",
    "        for block in [4]:\n",
    "            x = ConvBlockFunction4(\n",
    "                x,\n",
    "                params[f'conv{block}.0.weight'],\n",
    "                params[f'conv{block}.0.bias'],\n",
    "                params.get(f'conv{block}.1.weight'),\n",
    "                params.get(f'conv{block}.1.bias'))\n",
    "\n",
    "        x2 = x **2\n",
    "        x2_patch_sum = F.conv2d(x,params['ones'])\n",
    "\n",
    "        p2 = params['prototype_vectors'] ** 2\n",
    "        p2 = torch.sum(p2, dim=(1, 2, 3))\n",
    "        p2_reshape = p2.view(-1, 1, 1)\n",
    "        xp = F.conv2d(x,params['prototype_vectors'])\n",
    "        intermediate_result = - 2 * xp + p2_reshape  # use broadcast\n",
    "        # x2_patch_sum and intermediate_result are of the same shape\n",
    "        distances = F.relu(x2_patch_sum + intermediate_result)\n",
    "        \n",
    "        min_distances = -F.max_pool2d(-distances,\n",
    "                              kernel_size=(distances.size()[2],\n",
    "                                           distances.size()[3]))\n",
    "        min_distances = min_distances.view(-1, self.num_prototypes)\n",
    "        \n",
    "        prototype_activations = torch.log((min_distances + 1) / (min_distances + self.epsilon))\n",
    "        logits = F.linear(prototype_activations,params['logits.weight'])\n",
    "#         x = x.view(x.shape[0], -1)\n",
    "#         x = F.linear(x,\n",
    "#                      params['logits.weight'])\n",
    "#                      params['logits.bias'])\n",
    "        return logits,min_distances\n",
    "     \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "#         for m in self.add_on_layers.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 # every init technique has an underscore _ in the name\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "        self.set_last_layer_incorrect_connection(incorrect_strength=-0.5)\n",
    "    \n",
    "    \n",
    "    def push_forward(self, x,params):\n",
    "        '''this method is needed for the pushing operation'''\n",
    "        for block in [1, 2, 3]:\n",
    "            x = ConvBlockFunction(\n",
    "                x,\n",
    "                params[f'conv{block}.0.weight'],\n",
    "                params[f'conv{block}.0.bias'],\n",
    "                params.get(f'conv{block}.1.weight'),\n",
    "                params.get(f'conv{block}.1.bias'))\n",
    "        for block in [4]:\n",
    "            x = ConvBlockFunction4(\n",
    "                x,\n",
    "                params[f'conv{block}.0.weight'],\n",
    "                params[f'conv{block}.0.bias'],\n",
    "                params.get(f'conv{block}.1.weight'),\n",
    "                params.get(f'conv{block}.1.bias'))\n",
    "        conv_output=x\n",
    "        \n",
    "        x2 = x ** 2\n",
    "        x2_patch_sum = F.conv2d(input=x2, weight=params['ones'])\n",
    "        p2 = params['prototype_vectors'] ** 2\n",
    "        p2 = torch.sum(p2, dim=(1, 2, 3))\n",
    "        # p2 is a vector of shape (num_prototypes,)\n",
    "        # then we reshape it to (num_prototypes, 1, 1)\n",
    "        p2_reshape = p2.view(-1, 1, 1)\n",
    "\n",
    "        xp = F.conv2d(input=x, weight=params['prototype_vectors'])\n",
    "        intermediate_result = - 2 * xp + p2_reshape  # use broadcast\n",
    "        # x2_patch_sum and intermediate_result are of the same shape\n",
    "        distances = F.relu(x2_patch_sum + intermediate_result)\n",
    "        return conv_output, distances\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_init_new():\n",
    "    meta_multi = newClassifier(3, n_way).to(device)\n",
    "#     optimizer = torch.optim.Adam(meta_model.parameters(), \n",
    "#                                  lr=meta_lr)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    return meta_multi, loss_fn\n",
    "\n",
    "\n",
    "meta_multi, loss_fn = model_init_new()\n",
    "joint_optimizer_lrs = {'features': 1e-4,# 有改动，原来1e-4\n",
    "                       'add_on_layers': 3e-3,\n",
    "                       'prototype_vectors':1e-3}\n",
    "# no weight_decay!!!\n",
    "joint_optimizer_specs = \\\n",
    "[\n",
    "    {'params': meta_multi.conv1.parameters(), 'lr': joint_optimizer_lrs['features']}, # bias are now also being regularized\n",
    " {'params': meta_multi.conv2.parameters(), 'lr': joint_optimizer_lrs['features']},\n",
    " {'params': meta_multi.conv3.parameters(), 'lr': joint_optimizer_lrs['features']},\n",
    " {'params': meta_multi.conv4.parameters(), 'lr': joint_optimizer_lrs['features']},\n",
    "#  {'params': meta_multi.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers']},\n",
    " {'params': meta_multi.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "# {'params': meta_multi.logits.parameters(), 'lr':1e-4}\n",
    "]\n",
    "joint_optimizer = torch.optim.Adam(joint_optimizer_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_multi=torch.load('pp_10epoch_3max1stride_64ch_12_15_-2sep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper.py\n",
    "def makedir(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "\n",
    "def find_high_activation_crop(activation_map, percentile=60):\n",
    "    threshold = np.percentile(activation_map, percentile)\n",
    "    mask = np.ones(activation_map.shape)\n",
    "    mask[activation_map < threshold] = 0\n",
    "    lower_y, upper_y, lower_x, upper_x = 0, 0, 0, 0\n",
    "    for i in range(mask.shape[0]):\n",
    "        if np.amax(mask[i]) > 0.5:\n",
    "            lower_y = i\n",
    "            break\n",
    "    for i in reversed(range(mask.shape[0])):\n",
    "        if np.amax(mask[i]) > 0.5:\n",
    "            upper_y = i\n",
    "            break\n",
    "    for j in range(mask.shape[1]):\n",
    "        if np.amax(mask[:,j]) > 0.5:\n",
    "            lower_x = j\n",
    "            break\n",
    "    for j in reversed(range(mask.shape[1])):\n",
    "        if np.amax(mask[:,j]) > 0.5:\n",
    "            upper_x = j\n",
    "            break\n",
    "    return lower_y, upper_y+1, lower_x, upper_x+1\n",
    "\n",
    "def coumpute_zjym_prototype(size_ori,size_now,position):\n",
    "    patch=size_ori // size_now\n",
    "    fmap_height_start_index = position[1] * patch\n",
    "    fmap_height_end_index = (position[1]+1)*patch\n",
    "    fmap_width_start_index = position[2] * patch\n",
    "    fmap_width_end_index = (position[2]+1)*patch    \n",
    "    return [position[0],fmap_height_start_index,fmap_height_end_index,fmap_width_start_index,fmap_width_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimilarityConsis(search_batch_input,\n",
    "                       prototype_network_parallel,\n",
    "                       fast_weights,\n",
    "                       search_y=None, # required if class_specific == True\n",
    "                       num_classes=None):\n",
    "\n",
    "    search_batch = search_batch_input\n",
    "    with torch.no_grad():\n",
    "        search_batch = search_batch.cuda()\n",
    "        # this computation currently is not parallelized\n",
    "        _, proto_dist_torch = meta_multi.push_forward(search_batch,fast_weights)\n",
    "\n",
    "    proto_dist_ = np.copy(proto_dist_torch.detach().cpu().numpy())\n",
    "\n",
    "    del proto_dist_torch\n",
    "\n",
    "    class_to_img_index_dict = {key: [] for key in range(n_way)}\n",
    "        # img_y is the image's integer label\n",
    "    for img_index, img_y in enumerate(search_y):\n",
    "        img_label = img_y.item()\n",
    "        class_to_img_index_dict[img_label].append(img_index)\n",
    "\n",
    "    prototype_shape = meta_multi.prototype_shape\n",
    "    n_prototypes = prototype_shape[0]\n",
    "    x= list(np.zeros((q_query,n_prototypes)))\n",
    "    for j in range(n_prototypes):\n",
    "        target_class = torch.argmax(meta_multi.prototype_class_identity[j]).item()\n",
    "        proto_dist_j = proto_dist_[class_to_img_index_dict[target_class]][:,j,:,:]# class_to_img_index_dict[target_class]为原型所属的类，[:,j,:,:]为第j个原型。意思是这个类下的图片和第j个原型的结果。\n",
    "        for m in range(len(proto_dist_j)):\n",
    "            quer_proto_dist_j = proto_dist_j[m]\n",
    "            index=np.argmin(quer_proto_dist_j, axis=None)\n",
    "            x[m][j]=int(index)\n",
    "    res = 0\n",
    "    for i in range(len(x)):\n",
    "        if res ==0:\n",
    "            res=[[]]\n",
    "        else:\n",
    "            res.append([[]])\n",
    "        for j in range(len(x[0])):\n",
    "            res[i]=res[i]+[int(x[i][j])]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plusBB(img,x0,x1,y0,y1,piccurrent):\n",
    "#     fig = plt.figure(28)\n",
    "#     ax1 = plt.subplot(111)\n",
    "#     p = plt.imshow(img,vmin=0,vmax=1,shape=(28,28))\n",
    "#     upleft = (x0, y0)\n",
    "#     width = x1-x0\n",
    "#     height = y1-y0\n",
    "#     rect = patches.Rectangle(upleft, width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "#     ax1.add_patch(rect) \n",
    "#     ax1.axis('off')\n",
    "#     fig = plt.gcf()\n",
    "#     fig.savefig('sss2.png', dpi=1, transparent=True, pad_inches=0, bbox_inches='tight')\n",
    "# #     del fig,ax1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, train_iter = get_meta_batch(\n",
    "#             meta_batch_size, k_shot, q_query, \n",
    "#             train_loader, train_iter)\n",
    "# for task_index,meta_batch in enumerate(x):\n",
    "# #             print(task_index)\n",
    "#     support_set = meta_batch[: n_way * k_shot]  \n",
    "#     query_set = meta_batch[n_way * k_shot :]    \n",
    "\n",
    "#     fast_weights = OrderedDict(meta_multi.named_parameters())\n",
    "#     break\n",
    "# original_img_j=support_set[0]\n",
    "# original_img_j = original_img_j.data.cpu().numpy()\n",
    "# original_img_j = np.transpose(original_img_j, (1, 2, 0))\n",
    "# original_img_size = original_img_j.shape[0]\n",
    "# plusBB(original_img_j,5,10,10,20,'sss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prototypes_on_batch(search_batch_input,\n",
    "                               start_index_of_search_batch,\n",
    "                               prototype_network_parallel,\n",
    "                               global_min_proto_dist, # this will be updated\n",
    "                               global_min_fmap_patches, # this will be updated\n",
    "                               proto_rf_boxes, # this will be updated\n",
    "                               proto_bound_boxes, # this will be updated\n",
    "                               fast_weights,\n",
    "                               class_specific=True,\n",
    "                               search_y=None, # required if class_specific == True\n",
    "                               num_classes=None, # required if class_specific == True\n",
    "                               prototype_layer_stride=1,\n",
    "                               dir_for_saving_prototypes=None,\n",
    "                               prototype_img_filename_prefix=None,\n",
    "                               prototype_self_act_filename_prefix=None,\n",
    "                               prototype_activation_function_in_numpy=None):\n",
    "\n",
    "#     prototype_network_parallel.eval()\n",
    "\n",
    "\n",
    "        # print('preprocessing input for pushing ...')\n",
    "        # search_batch = copy.deepcopy(search_batch_input)\n",
    "    # 可能有问题\n",
    "#     search_batch = preprocess(search_batch_input)\n",
    "    search_batch = search_batch_input\n",
    "    with torch.no_grad():\n",
    "        search_batch = search_batch.cuda()\n",
    "        # this computation currently is not parallelized\n",
    "        protoL_input_torch, proto_dist_torch = prototype_network_parallel.push_forward(search_batch,fast_weights)\n",
    "\n",
    "    protoL_input_ = np.copy(protoL_input_torch.detach().cpu().numpy())\n",
    "    proto_dist_ = np.copy(proto_dist_torch.detach().cpu().numpy())\n",
    "\n",
    "    del protoL_input_torch, proto_dist_torch\n",
    "\n",
    "    if class_specific:\n",
    "        class_to_img_index_dict = {key: [] for key in range(num_classes)}\n",
    "        # img_y is the image's integer label\n",
    "        for img_index, img_y in enumerate(search_y):\n",
    "            img_label = img_y.item()\n",
    "            class_to_img_index_dict[img_label].append(img_index)\n",
    "\n",
    "    prototype_shape = prototype_network_parallel.prototype_shape\n",
    "    n_prototypes = prototype_shape[0]\n",
    "    proto_h = prototype_shape[2]\n",
    "    proto_w = prototype_shape[3]\n",
    "    max_dist = prototype_shape[1] * prototype_shape[2] * prototype_shape[3]\n",
    "\n",
    "    for j in range(n_prototypes):\n",
    "        #if n_prototypes_per_class != None:\n",
    "        if class_specific:\n",
    "            # target_class is the class of the class_specific prototype\n",
    "            target_class = torch.argmax(prototype_network_parallel.prototype_class_identity[j]).item()\n",
    "            # if there is not images of the target_class from this batch\n",
    "            # we go on to the next prototype\n",
    "            if len(class_to_img_index_dict[target_class]) == 0:\n",
    "                continue\n",
    "            proto_dist_j = proto_dist_[class_to_img_index_dict[target_class]][:,j,:,:]# class_to_img_index_dict[target_class]为原型所属的类，[:,j,:,:]为第j个原型。意思是这个类下的图片和第j个原型的结果。\n",
    "        else:\n",
    "            # if it is not class specific, then we will search through\n",
    "            # every example\n",
    "            proto_dist_j = proto_dist_[:,j,:,:]\n",
    "\n",
    "        batch_min_proto_dist_j = np.amin(proto_dist_j)# 第j个原型所属的第x类的最近的值。（激活最大的地方）\n",
    "        if batch_min_proto_dist_j < global_min_proto_dist[j]:# \n",
    "            batch_argmin_proto_dist_j = \\\n",
    "                list(np.unravel_index(np.argmin(proto_dist_j, axis=None),\n",
    "                                      proto_dist_j.shape))# batch最小原型距离的坐标。\n",
    "            if class_specific:\n",
    "                '''\n",
    "                change the argmin index from the index among\n",
    "                images of the target class to the index in the entire search\n",
    "                batch\n",
    "                '''\n",
    "                # 因为都是五分类，这一步可以省略。\n",
    "                batch_argmin_proto_dist_j[0] = class_to_img_index_dict[target_class][batch_argmin_proto_dist_j[0]]\n",
    "\n",
    "            # retrieve the corresponding feature map patch\n",
    "            img_index_in_batch = batch_argmin_proto_dist_j[0]\n",
    "            fmap_height_start_index = batch_argmin_proto_dist_j[1] * prototype_layer_stride\n",
    "            fmap_height_end_index = fmap_height_start_index + proto_h\n",
    "            fmap_width_start_index = batch_argmin_proto_dist_j[2] * prototype_layer_stride\n",
    "            fmap_width_end_index = fmap_width_start_index + proto_w\n",
    "            # 对应的features(x),对应的位置。\n",
    "            batch_min_fmap_patch_j = protoL_input_[img_index_in_batch,\n",
    "                                                   :,\n",
    "                                                   fmap_height_start_index:fmap_height_end_index,\n",
    "                                                   fmap_width_start_index:fmap_width_end_index]\n",
    "            # 此处应该会修改，毕竟每个五分类任务的训练集只有一个。\n",
    "            global_min_proto_dist[j] = batch_min_proto_dist_j\n",
    "            global_min_fmap_patches[j] = batch_min_fmap_patch_j\n",
    "            \n",
    "            # get the receptive field boundary of the image patch\n",
    "            # that generates the representation\n",
    "#             protoL_rf_info = prototype_network_parallel.proto_layer_rf_info\n",
    "#             rf_prototype_j = compute_rf_prototype(search_batch.size(2), batch_argmin_proto_dist_j, protoL_rf_info)\n",
    "            rf_prototype_j = coumpute_zjym_prototype(search_batch.size(2),protoL_input_.shape[2],batch_argmin_proto_dist_j)\n",
    "            # get the whole image\n",
    "            original_img_j = search_batch_input[rf_prototype_j[0]]\n",
    "            original_img_j = original_img_j.data.cpu().numpy()\n",
    "            original_img_j = np.transpose(original_img_j, (1, 2, 0))\n",
    "            original_img_size = original_img_j.shape[0]#28,28,1\n",
    "            \n",
    "            # crop out the receptive field\n",
    "            rf_img_j = original_img_j[rf_prototype_j[1]:rf_prototype_j[2],\n",
    "                                      rf_prototype_j[3]:rf_prototype_j[4], :]\n",
    "            \n",
    "            # save the prototype receptive field information\n",
    "            proto_rf_boxes[j, 0] = rf_prototype_j[0] + start_index_of_search_batch\n",
    "            proto_rf_boxes[j, 1] = rf_prototype_j[1]\n",
    "            proto_rf_boxes[j, 2] = rf_prototype_j[2]\n",
    "            proto_rf_boxes[j, 3] = rf_prototype_j[3]\n",
    "            proto_rf_boxes[j, 4] = rf_prototype_j[4]\n",
    "            if proto_rf_boxes.shape[1] == 6 and search_y is not None:# 记录类别。\n",
    "                proto_rf_boxes[j, 5] = search_y[rf_prototype_j[0]].item()\n",
    "\n",
    "            # find the highly activated region of the original image\n",
    "            proto_dist_img_j = proto_dist_[img_index_in_batch, j, :, :]\n",
    "            if prototype_network_parallel.prototype_activation_function == 'log':\n",
    "                proto_act_img_j = np.log((proto_dist_img_j + 1) / (proto_dist_img_j + prototype_network_parallel.epsilon))\n",
    "            elif prototype_network_parallel.prototype_activation_function == 'linear':\n",
    "                proto_act_img_j = max_dist - proto_dist_img_j\n",
    "            else:\n",
    "                proto_act_img_j = prototype_activation_function_in_numpy(proto_dist_img_j)\n",
    "            upsampled_act_img_j = cv2.resize(proto_act_img_j, dsize=(original_img_size, original_img_size),\n",
    "                                             interpolation=cv2.INTER_CUBIC)\n",
    "            proto_bound_j = find_high_activation_crop(upsampled_act_img_j)\n",
    "            # crop out the image patch with high activation as prototype image\n",
    "            proto_img_j = original_img_j[proto_bound_j[0]:proto_bound_j[1],\n",
    "                                         proto_bound_j[2]:proto_bound_j[3], :]\n",
    "\n",
    "            # save the prototype boundary (rectangular boundary of highly activated region)\n",
    "            proto_bound_boxes[j, 0] = proto_rf_boxes[j, 0]\n",
    "            proto_bound_boxes[j, 1] = proto_bound_j[0]\n",
    "            proto_bound_boxes[j, 2] = proto_bound_j[1]\n",
    "            proto_bound_boxes[j, 3] = proto_bound_j[2]\n",
    "            proto_bound_boxes[j, 4] = proto_bound_j[3]\n",
    "            if proto_bound_boxes.shape[1] == 6 and search_y is not None:\n",
    "                proto_bound_boxes[j, 5] = search_y[rf_prototype_j[0]].item()\n",
    "\n",
    "            if dir_for_saving_prototypes is not None:\n",
    "                if prototype_self_act_filename_prefix is not None:\n",
    "                    # save the numpy array of the prototype self activation\n",
    "                    np.save(os.path.join(dir_for_saving_prototypes,\n",
    "                                         prototype_self_act_filename_prefix + str(j) + '.npy'),\n",
    "                            proto_act_img_j)\n",
    "                if prototype_img_filename_prefix is not None:\n",
    "                    # save the whole image containing the prototype as png\n",
    "#                     plusBB(original_img_j,proto_bound_j[0],proto_bound_j[1],proto_bound_j[2],proto_bound_j[3],os.path.join(dir_for_saving_prototypes,\n",
    "#                                             prototype_img_filename_prefix + '-original' + str(j) + '.png'))\n",
    "#                     cv2.rectangle(original_img_j,(int(proto_bound_j[0]),int(proto_bound_j[1])),(int(proto_bound_j[2]),int(proto_bound_j[3])),(0,255,0),3)  \n",
    "                    plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "                                            prototype_img_filename_prefix + '-original' + str(j) + '.png'),\n",
    "                               original_img_j,\n",
    "#                                np.squeeze(original_img_j),\n",
    "                               vmin=0.0,\n",
    "                               vmax=1.0)\n",
    "                    # overlay (upsampled) self activation on original image and save the result\n",
    "                    rescaled_act_img_j = upsampled_act_img_j - np.amin(upsampled_act_img_j)\n",
    "                    rescaled_act_img_j = rescaled_act_img_j / np.amax(rescaled_act_img_j)\n",
    "                    heatmap = cv2.applyColorMap(np.uint8(255*rescaled_act_img_j), cv2.COLORMAP_JET)\n",
    "                    heatmap = np.float32(heatmap) / 255\n",
    "                    heatmap = heatmap[...,::-1]\n",
    "                    overlayed_original_img_j = 0.5 * original_img_j + 0.3 * heatmap\n",
    "                    plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "                                            prototype_img_filename_prefix + '-original_with_self_act' + str(j) + '.png'),\n",
    "                               overlayed_original_img_j,\n",
    "                               vmin=0.0,\n",
    "                               vmax=1.0)\n",
    "                    \n",
    "                    # if different from the original (whole) image, save the prototype receptive field as png\n",
    "                    if rf_img_j.shape[0] != original_img_size or rf_img_j.shape[1] != original_img_size:\n",
    "                        plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "                                                prototype_img_filename_prefix + '-receptive_field' + str(j) + '.png'),\n",
    "#                                    np.squeeze(rf_img_j),\n",
    "                                   rf_img_j,\n",
    "                                   vmin=0.0,\n",
    "                                   vmax=1.0)\n",
    "                        overlayed_rf_img_j = overlayed_original_img_j[rf_prototype_j[1]:rf_prototype_j[2],\n",
    "                                                                      rf_prototype_j[3]:rf_prototype_j[4]]\n",
    "                        plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "                                                prototype_img_filename_prefix + '-receptive_field_with_self_act' + str(j) + '.png'),\n",
    "                                   overlayed_rf_img_j,\n",
    "                                   vmin=0.0,\n",
    "                                   vmax=1.0)\n",
    "                    \n",
    "                    # save the prototype image (highly activated region of the whole image)\n",
    "                    plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "                                            prototype_img_filename_prefix + str(j) + '.png'),\n",
    "#                                 np.squeeze(proto_img_j),\n",
    "                                proto_img_j,\n",
    "                               vmin=0.0,\n",
    "                               vmax=1.0)\n",
    "                \n",
    "    if class_specific:\n",
    "        del class_to_img_index_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def toList(tensor):\n",
    "    res = np.copy(tensor.detach().cpu().numpy())\n",
    "    return res.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_multi = torch.load('best2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MetaAlgorithmGeneratorNew(\n",
    "    inner_update      = inner_update_MAML, \n",
    "    collect_gradients = collect_gradients_MAML, \n",
    "    outer_update      = outer_update_MAML):\n",
    "\n",
    "    global calculate_accuracy\n",
    "\n",
    "    def MetaAlgorithmNew(\n",
    "        model, optimizer, x, n_way, k_shot, q_query, loss_fn,epoch_number,step,\n",
    "        inner_train_step=8, inner_lr=0.04, train=0): \n",
    "        criterion = loss_fn\n",
    "        task_loss, task_acc = [], []\n",
    "        special_grad = OrderedDict()  # Added for variants!\n",
    "        coefs = {\n",
    "            'crs_ent': 1,\n",
    "            'clst': 0.8,\n",
    "            'sep': -0.8,\n",
    "            'l1': 1e-4,\n",
    "            }\n",
    "        task_crsEnt,task_cls,task_sep,task_l1 = [],[],[],[]\n",
    "        task_dic={}\n",
    "        task_dic_path='./test'\n",
    "        makedir(task_dic_path)\n",
    "        for task_index,meta_batch in enumerate(x):\n",
    "#             print(task_index)\n",
    "            support_set = meta_batch[: n_way * k_shot]  \n",
    "            query_set = meta_batch[n_way * k_shot :]    \n",
    "            \n",
    "            fast_weights = OrderedDict(model.named_parameters())\n",
    "            \n",
    "            ### ---------- INNER TRAIN LOOP ---------- ###\n",
    "            for inner_step in range(inner_train_step): \n",
    "                train_label = create_label(n_way, k_shot).to(device)\n",
    "                logits,min_distances = model.functional_forward(support_set, fast_weights)\n",
    "                cross_entropy = criterion(logits, train_label)\n",
    "\n",
    "                max_dist = (model.prototype_shape[1]\n",
    "                            * model.prototype_shape[2]\n",
    "                            * model.prototype_shape[3])\n",
    "                # calculate cluster cost                \n",
    "                prototypes_of_correct_class = torch.t(model.prototype_class_identity[:,train_label]).cuda()\n",
    "                inverted_distances, _ = torch.max((max_dist - min_distances) * prototypes_of_correct_class, dim=1)\n",
    "                cluster_cost = torch.mean(max_dist - inverted_distances)\n",
    "                \n",
    "                \n",
    "                # calculate separation cost\n",
    "                prototypes_of_wrong_class = 1 - prototypes_of_correct_class\n",
    "                inverted_distances_to_nontarget_prototypes, _ = \\\n",
    "                    torch.max((max_dist - min_distances) * prototypes_of_wrong_class, dim=1)\n",
    "                separation_cost = torch.mean(max_dist - inverted_distances_to_nontarget_prototypes)\n",
    "                    \n",
    "                # calculate avg cluster cost\n",
    "                avg_separation_cost = \\\n",
    "                    torch.sum(min_distances * prototypes_of_wrong_class, dim=1) / torch.sum(prototypes_of_wrong_class, dim=1)\n",
    "                avg_separation_cost = torch.mean(avg_separation_cost)\n",
    "                \n",
    "                \n",
    "                l1_mask = 1 - torch.t(model.prototype_class_identity).cuda()\n",
    "                l1 = (model.logits.weight * l1_mask).norm(p=1)\n",
    "                loss = (coefs['crs_ent'] * cross_entropy\n",
    "                                  + coefs['clst'] * cluster_cost\n",
    "                                  + coefs['sep'] * separation_cost\n",
    "                                  + coefs['l1'] * l1)\n",
    "                ones =  fast_weights.pop('ones')\n",
    "                logitstemp = fast_weights.pop('logits.weight')\n",
    "                fast_weights = inner_update(fast_weights, loss, inner_lr)\n",
    "                \n",
    "                fast_weights['ones'] = ones\n",
    "                fast_weights['logits.weight'] = logitstemp\n",
    "            \n",
    "            # push prototype\n",
    "            if epoch_number>14:\n",
    "                train_label = create_label(n_way,k_shot).to(device)\n",
    "\n",
    "                prototype_shape = model.prototype_shape\n",
    "                n_prototypes = model.num_prototypes\n",
    "                global_min_proto_dist = np.full(n_prototypes, np.inf)\n",
    "                global_min_fmap_patches = np.zeros(\n",
    "                    [n_prototypes,\n",
    "                     prototype_shape[1],\n",
    "                     prototype_shape[2],\n",
    "                     prototype_shape[3]])\n",
    "                proto_rf_boxes = np.full(shape=[n_prototypes, 6],\n",
    "                                    fill_value=-1)\n",
    "                proto_bound_boxes = np.full(shape=[n_prototypes, 6],\n",
    "                                            fill_value=-1)\n",
    "                makedir('./img')\n",
    "                root_dir_for_saving_prototypes = os.path.join('./img', 'epoch-'+str(epoch_number))\n",
    "                task_dic_path = os.path.join(task_dic_path,'epoch-'+str(epoch_number))\n",
    "                if train==0:\n",
    "                    root_dir_for_saving_prototypes = os.path.join(root_dir_for_saving_prototypes, 'train')\n",
    "                elif train==1:\n",
    "                    root_dir_for_saving_prototypes = os.path.join(root_dir_for_saving_prototypes, 'val')\n",
    "                else:\n",
    "                    root_dir_for_saving_prototypes = os.path.join('./test','epoch-'+str(epoch_number))\n",
    "                    root_dir_for_saving_prototypes =os.path.join(root_dir_for_saving_prototypes,'prototype')\n",
    "                root_dir_for_saving_prototypes = os.path.join(root_dir_for_saving_prototypes,'step-'+str(step))\n",
    "                \n",
    "                makedir(root_dir_for_saving_prototypes)\n",
    "                # 此处的epoch_number可能要传epoch进来\n",
    "#                 print(os.path.join(root_dir_for_saving_prototypes,'task-'+str(task_index)))\n",
    "                proto_epoch_dir =os.path.join(root_dir_for_saving_prototypes,'task-'+str(task_index))\n",
    "                \n",
    "                makedir(proto_epoch_dir)\n",
    "                search_batch_size =n_way\n",
    "\n",
    "                num_classes = model.num_classes\n",
    "    #             for push_iter, search_batch_inpu in eupdate_prototypes_on_batchnumerate(support_set):\n",
    "\n",
    "                start_index_of_search_batch = 0\n",
    "\n",
    "                update_prototypes_on_batch(support_set,\n",
    "                                   start_index_of_search_batch,\n",
    "                                   model,\n",
    "                                   global_min_proto_dist,\n",
    "                                   global_min_fmap_patches,\n",
    "                                   proto_rf_boxes,\n",
    "                                   proto_bound_boxes,\n",
    "                                   fast_weights,\n",
    "                                   class_specific=True,\n",
    "                                   search_y=train_label,\n",
    "                                   num_classes=num_classes,\n",
    "                                   prototype_layer_stride=1,\n",
    "                                   dir_for_saving_prototypes=proto_epoch_dir,\n",
    "                                   prototype_img_filename_prefix='prototype-img',\n",
    "                                   prototype_self_act_filename_prefix='prototype-self-act')    \n",
    "\n",
    "                proto_bound_boxes_filename_prefix='bb'\n",
    "                # proto_bound_boxes_filename_prefix ='bb'=='None'\n",
    "                if proto_epoch_dir != None and proto_bound_boxes_filename_prefix != None:\n",
    "                    np.save(os.path.join(proto_epoch_dir, proto_bound_boxes_filename_prefix + str(epoch_number) + '.npy'),\n",
    "                            proto_bound_boxes)     \n",
    "\n",
    "                prototype_update = np.reshape(global_min_fmap_patches,\n",
    "                                      tuple(prototype_shape))\n",
    "                \n",
    "                fast_weights['prototype_vectors'] = torch.tensor(prototype_update, dtype=torch.float32).cuda()\n",
    "            \n",
    "            \n",
    "            \n",
    "            ### ---------- INNER VALID LOOP ---------- ###\n",
    "            # ----inner test-----#\n",
    "            val_label = create_label(n_way, q_query).to(device)\n",
    "            # FIXME: W for val?\n",
    "            ones =  fast_weights.pop('ones')            \n",
    "            logitstemp = fast_weights.pop('logits.weight')\n",
    "            \n",
    "            special_grad = collect_gradients(\n",
    "                special_grad, fast_weights, model, len(x))\n",
    "            \n",
    "            fast_weights['ones'] = ones\n",
    "            fast_weights['logits.weight'] = logitstemp\n",
    "            # Collect gradients for outer loop\n",
    "            logits,min_distances = model.functional_forward(query_set, fast_weights) \n",
    "            \n",
    "            cross_entropy = criterion(logits, val_label)\n",
    "            all_dic={}\n",
    "            index_dic={}\n",
    "            high_dic={}\n",
    "            if train ==2:\n",
    "                if epoch_number>14:\n",
    "                    task_dic_path = os.path.join('./test','epoch-'+str(epoch_number))\n",
    "                    task_dic_path = os.path.join(task_dic_path,'original')\n",
    "                    task_dic_path = os.path.join(task_dic_path,'step-'+str(step))\n",
    "                    task_dic_path = os.path.join(task_dic_path,'task-'+str(task_index))\n",
    "                    makedir(task_dic_path)\n",
    "    #                 pp_distance=min_distances.cpu().numpy()\n",
    "                    for t in range(len(val_label)):\n",
    "                        original_img_j = query_set[t]\n",
    "                        original_img_j = original_img_j.data.cpu().numpy()\n",
    "                        original_img_j = np.transpose(original_img_j, (1, 2, 0))\n",
    "                        original_img_size = original_img_j.shape[0]\n",
    "                        if t % q_query ==0:\n",
    "                            final_path = os.path.join(task_dic_path,'class-'+str(t//q_query))\n",
    "                            makedir(final_path)\n",
    "                            \n",
    "                        plt.imsave(os.path.join(final_path,str(t%q_query)+'.png'),\n",
    "                               original_img_j,\n",
    "                               vmin=0.0,\n",
    "                               vmax=1.0)\n",
    "                        \n",
    "                    task_dic['task-'+str(task_index)]=toList(min_distances)\n",
    "                    all_dic['res']=predict_true(logits,val_label)\n",
    "                    high_index= SimilarityConsis(query_set,model,fast_weights,search_y=val_label,num_classes=num_classes)\n",
    "                    index_dic['index']=high_index\n",
    "                    high_score =  torch.log((min_distances + 1) / (min_distances + model.epsilon))\n",
    "                    high_dic['res']=toList(high_score)\n",
    "                    jsondata = json.dumps(task_dic,indent=4)\n",
    "                    jsondata2 = json.dumps(all_dic,indent=4)\n",
    "                    jsondata3 = json.dumps(index_dic,indent=4)\n",
    "                    jsondata4 = json.dumps(high_dic,indent=4)\n",
    "                    f = open(task_dic_path+'/distance.json', 'w')\n",
    "                    f2= open(task_dic_path+'/predict_res.json','w')\n",
    "                    f3 =open(task_dic_path+'/index.json','w')\n",
    "                    f4=open(task_dic_path+'/activation.json','w')\n",
    "                    f.write(jsondata)\n",
    "                    f2.write(jsondata2)\n",
    "                    f3.write(jsondata3)\n",
    "                    f4.write(jsondata4)\n",
    "                    f.close()\n",
    "                    f2.close()\n",
    "                    f3.close()\n",
    "                    f4.close()\n",
    "            max_dist = (model.prototype_shape[1]\n",
    "                * model.prototype_shape[2]\n",
    "                * model.prototype_shape[3])\n",
    "            \n",
    "            # calculate cluster cost                \n",
    "            prototypes_of_correct_class = torch.t(model.prototype_class_identity[:,val_label]).cuda()\n",
    "            inverted_distances, _ = torch.max((max_dist - min_distances) * prototypes_of_correct_class, dim=1)\n",
    "            cluster_cost = torch.mean(max_dist - inverted_distances)\n",
    "\n",
    "\n",
    "            # calculate separation cost\n",
    "            prototypes_of_wrong_class = 1 - prototypes_of_correct_class\n",
    "            inverted_distances_to_nontarget_prototypes, _ = \\\n",
    "                torch.max((max_dist - min_distances) * prototypes_of_wrong_class, dim=1)\n",
    "            separation_cost = torch.mean(max_dist - inverted_distances_to_nontarget_prototypes)\n",
    "\n",
    "            # calculate avg cluster cost\n",
    "            avg_separation_cost = \\\n",
    "                torch.sum(min_distances * prototypes_of_wrong_class, dim=1) / torch.sum(prototypes_of_wrong_class, dim=1)\n",
    "            avg_separation_cost = torch.mean(avg_separation_cost)\n",
    "            l1_mask = 1 - torch.t(model.prototype_class_identity).cuda()\n",
    "            l1 = (model.logits.weight * l1_mask).norm(p=1)\n",
    "            \n",
    "            loss = (coefs['crs_ent'] * cross_entropy\n",
    "                              + coefs['clst'] * cluster_cost\n",
    "                              + coefs['sep'] * separation_cost\n",
    "                              + coefs['l1'] * l1)\n",
    "            \n",
    "            task_loss.append(loss)\n",
    "            task_acc.append(calculate_accuracy(logits, val_label))\n",
    "            task_crsEnt.append(cross_entropy)\n",
    "            task_cls.append(cluster_cost)\n",
    "            task_sep.append(separation_cost)\n",
    "            task_l1.append(l1)\n",
    "#             task_acc.append(calculate_accuracy(logits, val_label))\n",
    "        # Update outer loop\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        meta_batch_loss = torch.stack(task_loss).mean()\n",
    "        meta_batch_crsEnt=torch.stack(task_crsEnt).mean()\n",
    "        meta_batch_cls =torch.stack(task_cls).mean()\n",
    "        meta_batch_sep = torch.stack(task_sep).mean()\n",
    "        meta_batch_l1 = torch.stack(task_l1).mean()\n",
    "        if train==0:\n",
    "            ones =  fast_weights.pop('ones')\n",
    "            logitstemp = fast_weights.pop('logits.weight')\n",
    "            # Notice the update part!\n",
    "            outer_update(model, meta_batch_loss, special_grad)\n",
    "            fast_weights['ones'] = ones\n",
    "            fast_weights['logits.weight'] = logitstemp            \n",
    "            optimizer.step()\n",
    "        task_acc = np.mean(task_acc)\n",
    "        return meta_batch_loss, task_acc,meta_batch_crsEnt,meta_batch_cls,meta_batch_sep,meta_batch_l1\n",
    "    return MetaAlgorithmNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAML_new = MetaAlgorithmGeneratorNew()\n",
    "MetaAlgorithm_new =MAML_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(\"ssss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_epoch=20\n",
    "# for epoch in range(max_epoch):\n",
    "#     print(\"Epoch %d\" % (epoch + 1))\n",
    "#     train_meta_loss = []\n",
    "#     train_acc = []\n",
    "#     train_meta_crsEnt,train_meta_cls,train_meta_sep,train_meta_l1=[],[],[],[]\n",
    "#     # The \"step\" here is a meta-gradinet update step\n",
    "#     for step in tqdm(range(\n",
    "#             len(train_loader) // meta_batch_size)): \n",
    "#         x, train_iter = get_meta_batch(\n",
    "#             meta_batch_size, k_shot, q_query, \n",
    "#             train_loader, train_iter)\n",
    "# #         meta_loss, acc = MetaAlgorithm_new(\n",
    "#         meta_loss, acc,meta_crsEnt,meta_cls,meta_sep,meta_l1 = MetaAlgorithm_new(\n",
    "#             meta_multi, joint_optimizer, x, \n",
    "#             n_way, k_shot, q_query, loss_fn,epoch,step)\n",
    "#         train_meta_loss.append(meta_loss.item())\n",
    "#         train_acc.append(acc)\n",
    "#         train_meta_crsEnt.append(meta_crsEnt.item())\n",
    "#         train_meta_cls.append(meta_cls.item())\n",
    "#         train_meta_sep.append(meta_sep.item())\n",
    "#         train_meta_l1.append(meta_l1.item())\n",
    "\n",
    "#     print(\"  Loss    : \", \"%.3f\" % (np.mean(train_meta_loss)), end='\\t')\n",
    "#     print(\"  Accuracy: \", \"%.3f %%\" % (np.mean(train_acc) * 100),end='\\t')\n",
    "#     print(\"  crsEnt    : \", \"%.3f\" % (np.mean(train_meta_crsEnt)), end='\\t')\n",
    "#     print(\"  cls    : \", \"%.3f\" % (np.mean(train_meta_cls)), end='\\t')\n",
    "#     print(\"  sep    : \", \"%.3f\" % (np.mean(train_meta_sep)), end='\\t')\n",
    "#     print(\"  l1    : \", \"%.3f\" % (np.mean(train_meta_l1)))\n",
    "\n",
    "# #     See the validation accuracy after each epoch.\n",
    "# #     Early stopping is welcomed to implement.\n",
    "#     val_acc = []\n",
    "#     for eval_step in tqdm(range(\n",
    "#             len(val_loader) // (eval_batches))):\n",
    "#         x, val_iter = get_meta_batch(\n",
    "#             eval_batches, k_shot, q_query, \n",
    "#             val_loader, val_iter)\n",
    "#         # We update three inner steps when testing.\n",
    "# #         _, acc = MetaAlgorithm_new(meta_multi, joint_optimizer, x, \n",
    "#         _, acc,_,_,_,_ = MetaAlgorithm_new(meta_multi, joint_optimizer, x, \n",
    "#             n_way, k_shot, q_query, \n",
    "#                       loss_fn,\n",
    "#                       epoch,\n",
    "#                       eval_step,\n",
    "#                       inner_train_step=12, \n",
    "#                       train=False) \n",
    "#         val_acc.append(acc)\n",
    "#     print(\"  Validation accuracy: \", \"%.3f %%\" % (np.mean(val_acc) * 100))\n",
    "#     if np.mean(val_acc)>0.98:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m6/\u001b[93m6\u001b[0m\u001b[0m   \u001b[99m00:00:29<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.20it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acb15973d82454ab4c445f6aa84a9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing accuracy:  95.656 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_acc = []\n",
    "for test_step in tqdm(range(\n",
    "        len(test_loader) // (test_batches))):\n",
    "    x, test_iter = get_meta_batch(\n",
    "        test_batches, k_shot, q_query, \n",
    "        test_loader, test_iter)\n",
    "    # When testing, we update 3 inner-steps\n",
    "    _, acc,_,_,_,_ = MetaAlgorithm_new(meta_multi, joint_optimizer, x, \n",
    "            n_way, k_shot, q_query, \n",
    "                      loss_fn,\n",
    "                      27,\n",
    "                      test_step,\n",
    "                      inner_train_step=10, \n",
    "                      train=2) \n",
    "    test_acc.append(acc)\n",
    "print(\"  Testing accuracy: \", \"%.3f %%\" % (np.mean(test_acc) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(meta_multi,'完全体')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(meta_multi,'完全体_3max1stride_noadd_64ch_10_12_20prototype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(meta_multi,'完全体3max1stride_noadd_64ch_10_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_multi= torch.load('完全体3max1stride_noadd_64ch_10_12')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
