{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UU9VjAXB037b",
    "outputId": "4c0cdc76-77ba-492d-ac72-2bc8b6f8af33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import qqdm\n",
    "except:\n",
    "    ! pip install qqdm > /dev/null 2>&1\n",
    "print(\"Done!\")\n",
    "from PIL import Image\n",
    "import csv\n",
    "from IPython.display import display\n",
    "import glob, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "try:\n",
    "    from qqdm.notebook import qqdm as tqdm\n",
    "except ModuleNotFoundError:\n",
    "    from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import matplotlib.patches as patches\n",
    "vgg16_conv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92JFQoQFpjXi",
    "outputId": "c4d51b76-442a-40d9-e293-e96011e5df6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 17 21:40:28 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:10:00.0 Off |                  Off |\n",
      "| 30%   26C    P5    22W / 300W |      0MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0U6DHHXxMG-"
   },
   "source": [
    "# 新段落"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XTjQSQXIxQOD"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nfoXgiH_07sv"
   },
   "outputs": [],
   "source": [
    "# workspace_dir = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gvN89b671jMO"
   },
   "outputs": [],
   "source": [
    "# # # # # # Use `unzip' command to decompress\n",
    "# !unzip '/content/drive/MyDrive/mini-imagenet (2).zip' -d '/content/miniImageNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kkmgyqGylK7f"
   },
   "outputs": [],
   "source": [
    "class Vgg16Conv(nn.Module):  # conv搭建方法介绍：https://blog.csdn.net/qq_38607066/article/details/98517116\n",
    "\n",
    "    def __init__(self, num_cls=1000):\n",
    "        super(Vgg16Conv, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(  # 序贯模型 一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，同时以神经网络模块为元素的有序字典也可以作为传入参数。\n",
    "            # conv1\n",
    "            nn.Conv2d(3, 64, 3, padding=1),  # input output kernal //padding=1,那么就会在原来输入层的基础上,上下左右各补一行\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),  # return_indices：True-记录输入图片池化时最大值的位置，之后反卷积中可以使用\n",
    "\n",
    "            # conv2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv3\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv4\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv5\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(  # 全连接层 分类器\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_cls),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.conv_layer_indices = [0, 2, 5, 7, 10, 12, 14, 17, 19, 21, 24, 26, 28]  # 卷积层所在层数\n",
    "        self.feature_maps = OrderedDict()  # 有序字典，之后用于存储中间输出结果\n",
    "        self.pool_locs = OrderedDict()  # 用于存储图片最大池化过程中最大值所在位置\n",
    "\n",
    "        self.init_weights()  # 调用权重初始化函数，将VGG16模型的参数导入\n",
    "\n",
    "    def init_weights(self):  # 将vgg16预训练模型的每层权重赋予此模型中（即自己搭建一个vgg16模型的框架，使用预训练模型的权重）\n",
    "        vgg16_pretrained = models.vgg16(pretrained=True)\n",
    "        # 卷积部分赋权值\n",
    "        for idx, layer in enumerate(vgg16_pretrained.features):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.features[idx].weight.data = layer.weight.data  # 权重 weight\n",
    "                self.features[idx].bias.data = layer.bias.data  # 偏置 bias\n",
    "        # 全连接部分赋权值\n",
    "        for idx, layer in enumerate(vgg16_pretrained.classifier):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                self.classifier[idx].weight.data = layer.weight.data\n",
    "                self.classifier[idx].bias.data = layer.bias.data\n",
    "\n",
    "    def check(self):\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):  # 前向传播过程\n",
    "        for idx, layer in enumerate(self.features):\n",
    "            if isinstance(layer, nn.MaxPool2d):  # 在卷积部分的池化时，记录下池化过程中最大值位置\n",
    "                x, location = layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        x = x.view(x.size()[0], -1)  # 把数据size变成(1, 512 * 7 * 7)，将数据平铺成1维（相当于flatten层）# .view方法中，-1只有一个，表示该维度值自适应计算得\n",
    "\n",
    "        output = self.classifier(x)  # 平铺数据后输入全连接层\n",
    "        return output\n",
    "    def conv_forward(self, x):  # 前向传播过程\n",
    "        for idx, layer in enumerate(self.features):\n",
    "            if isinstance(layer, nn.MaxPool2d):  # 在卷积部分的池化时，记录下池化过程中最大值位置\n",
    "                x, location = layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        # x = x.view(x.size()[0], -1)  # 把数据size变成(1, 512 * 7 * 7)，将数据平铺成1维（相当于flatten层）# .view方法中，-1只有一个，表示该维度值自适应计算得\n",
    "\n",
    "        # output = self.classifier(x)  # 平铺数据后输入全连接层\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7vELKEt-037g"
   },
   "outputs": [],
   "source": [
    "workspace_dir = '.'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# fix random seeds\n",
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vXtPny2O037h"
   },
   "outputs": [],
   "source": [
    "def predict_conv(img):\n",
    "    global vgg16_conv\n",
    "    if vgg16_conv is None:\n",
    "        vgg16_conv = Vgg16Conv()\n",
    "        vgg16_conv.eval()\n",
    "        vgg16_conv.to(device)\n",
    "    return vgg16_conv.conv_forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EV4-i7zUUhPI"
   },
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    global vgg16_conv\n",
    "    if vgg16_conv is None:\n",
    "        vgg16_conv = Vgg16Conv()\n",
    "        vgg16_conv.eval()\n",
    "        vgg16_conv.to(device)\n",
    "    return vgg16_conv.forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aat22ydI037i"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_ch, k_way):\n",
    "        super(Classifier, self).__init__()\n",
    "        # vgg16_conv=Vgg16Conv()\n",
    "        # self.vgg16_conv = vgg16_conv\n",
    "        # self.vgg16_conv.eval()\n",
    "        # self.vgg16_conv.to(device)\n",
    "        # store(vgg16_conv)\n",
    "        # self._initialize_weights()\n",
    "        self.logits=nn.Linear(512*7*7, 5,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.vgg16_conv(x)\n",
    "        # x = predict_conv(x)\n",
    "        # x = self.logits(x)\n",
    "        return predict(x)\n",
    "\n",
    "    def functional_forward(self, x, params):\n",
    "        '''\n",
    "        Arguments:\n",
    "        x: input images [batch, 1, 28, 28]\n",
    "        params: model parameters, \n",
    "                i.e. weights and biases of convolution\n",
    "                      and weights and biases of \n",
    "                                    batch normalization\n",
    "                type is an OrderedDict\n",
    "\n",
    "        Arguments:\n",
    "        x: input images [batch, 1, 28, 28]\n",
    "        params: The model parameters, \n",
    "                i.e. weights and biases of convolution \n",
    "                      and batch normalization layers\n",
    "                It's an `OrderedDict`\n",
    "        '''\n",
    "        # self.vgg16_conv(x)\n",
    "        x = predict_conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.linear(x,\n",
    "                         params['logits.weight'])\n",
    "                        #  params['logits.bias'])\n",
    "        # return predict(x)\n",
    "        return x\n",
    "\n",
    "        # return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yjWA2pouCBo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w1nfSvX6037j"
   },
   "outputs": [],
   "source": [
    "# class newClassifier(nn.Module):\n",
    "#   def __init__(self, in_ch, k_way,prototype_shape,img_size,epsilon,prototype_activation_function):\n",
    "#     super(newClassifier, self).__init__()\n",
    "#     self.conv1 = ConvBlock(in_ch, 64)\n",
    "#     self.conv2 = ConvBlock(64, 64)\n",
    "#     self.conv3 = ConvBlock(64, 64)\n",
    "#     self.conv4 = ConvBlock(64, 64)\n",
    "#     # self.conv4 = ConvBlock4(64, 64)\n",
    "# #         self.add_on_layers = nn.Sequential(\n",
    "# #             nn.Conv2d(64,128,kernel_size=1),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Conv2d(128,128,kernel_size=1),\n",
    "# #             nn.Sigmoid()\n",
    "# #         )\n",
    "#     prototype_shape = prototype_shape\n",
    "#     self.prototype_shape = prototype_shape\n",
    "#     self.img_size = img_size\n",
    "#     self.num_prototypes = prototype_shape[0]\n",
    "#     self.num_classes = k_way\n",
    "#     self.epsilon = epsilon\n",
    "#     self.prototype_activation_function =  prototype_activation_function\n",
    "#     assert(self.num_prototypes % self.num_classes == 0)\n",
    "#     self.prototype_class_identity = torch.zeros(self.num_prototypes,\n",
    "#                           self.num_classes)\n",
    "#     num_prototypes_per_class = self.num_prototypes // self.num_classes\n",
    "#     for j in range(self.num_prototypes):\n",
    "#         self.prototype_class_identity[j, j // num_prototypes_per_class] = 1\n",
    "        \n",
    "#     self.prototype_vectors = nn.Parameter(torch.rand(self.prototype_shape),\n",
    "#                                           requires_grad=True)\n",
    "#     self.ones = nn.Parameter(torch.ones(self.prototype_shape),\n",
    "#                               requires_grad=False)\n",
    "#     self.logits = nn.Linear(self.num_prototypes, k_way\n",
    "#                                 ,bias=False\n",
    "#                             ) # do not use bias    \n",
    "#     self._initialize_weights()\n",
    "  \n",
    "  \n",
    "  \n",
    "#   def forward(self, x):\n",
    "#     x = self.conv1(x)\n",
    "#     x = self.conv2(x)\n",
    "#     x = self.conv3(x)\n",
    "#     x = self.conv4(x)\n",
    "#     x = x.view(x.shape[0], -1)\n",
    "#     x = self.logits(x)\n",
    "#     return x\n",
    "\n",
    "  \n",
    "#   def set_last_layer_incorrect_connection(self, incorrect_strength):\n",
    "#     '''\n",
    "#     the incorrect strength will be actual strength if -0.5 then input -0.5\n",
    "#     '''\n",
    "#     positive_one_weights_locations = torch.t(self.prototype_class_identity)\n",
    "#     negative_one_weights_locations = 1 - positive_one_weights_locations\n",
    "\n",
    "#     correct_class_connection = 1\n",
    "#     incorrect_class_connection = incorrect_strength\n",
    "#     self.logits.weight.data.copy_(\n",
    "#         correct_class_connection * positive_one_weights_locations\n",
    "#         + incorrect_class_connection * negative_one_weights_locations)\n",
    "\n",
    "  \n",
    "#   def functional_forward(self, x, params):\n",
    "#     '''\n",
    "#     Arguments:\n",
    "#     x: input images [batch, 1, 28, 28]\n",
    "#     params: model parameters, \n",
    "#             i.e. weights and biases of convolution\n",
    "#                   and weights and biases of \n",
    "#                                 batch normalization\n",
    "#             type is an OrderedDict\n",
    "\n",
    "#     Arguments:\n",
    "#     x: input images [batch, 1, 28, 28]\n",
    "#     params: The model parameters, \n",
    "#             i.e. weights and biases of convolution \n",
    "#                   and batch normalization layers\n",
    "#             It's an `OrderedDict`\n",
    "#     '''\n",
    "    \n",
    "#     # for block in [1, 2,3,4]:\n",
    "#     #     x = ConvBlockFunction(\n",
    "#     #         x,\n",
    "#     #         params[f'conv{block}.0.weight'],\n",
    "#     #         params[f'conv{block}.0.bias'],\n",
    "#     #         params.get(f'conv{block}.1.weight'),\n",
    "#     #         params.get(f'conv{block}.1.bias'))\n",
    "#     # for block in [4]:\n",
    "#     #     x = ConvBlockFunction4(\n",
    "#     #         x,\n",
    "#     #         params[f'conv{block}.0.weight'],\n",
    "#     #         params[f'conv{block}.0.bias'],\n",
    "#     #         params.get(f'conv{block}.1.weight'),\n",
    "#     #         params.get(f'conv{block}.1.bias'))\n",
    "\n",
    "#     x2 = x **2\n",
    "#     x2_patch_sum = F.conv2d(x,params['ones'])\n",
    "\n",
    "#     p2 = params['prototype_vectors'] ** 2\n",
    "#     p2 = torch.sum(p2, dim=(1, 2, 3))\n",
    "#     p2_reshape = p2.view(-1, 1, 1)\n",
    "#     xp = F.conv2d(x,params['prototype_vectors'])\n",
    "#     intermediate_result = - 2 * xp + p2_reshape  # use broadcast\n",
    "#     # x2_patch_sum and intermediate_result are of the same shape\n",
    "#     distances = F.relu(x2_patch_sum + intermediate_result)\n",
    "    \n",
    "#     min_distances = -F.max_pool2d(-distances,\n",
    "#                           kernel_size=(distances.size()[2],\n",
    "#                                         distances.size()[3]))\n",
    "#     min_distances = min_distances.view(-1, self.num_prototypes)\n",
    "    \n",
    "#     prototype_activations = torch.log((min_distances + 1) / (min_distances + self.epsilon))\n",
    "#     logits = F.linear(prototype_activations,params['logits.weight'])\n",
    "# #         x = x.view(x.shape[0], -1)\n",
    "# #         x = F.linear(x,\n",
    "# #                      params['logits.weight'])\n",
    "# #                      params['logits.bias'])\n",
    "#     return logits,min_distances\n",
    "    \n",
    "  \n",
    "#   def _initialize_weights(self):\n",
    "# #         for m in self.add_on_layers.modules():\n",
    "# #             if isinstance(m, nn.Conv2d):\n",
    "# #                 # every init technique has an underscore _ in the name\n",
    "# #                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# #                 if m.bias is not None:\n",
    "# #                     nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# #             elif isinstance(m, nn.BatchNorm2d):\n",
    "# #                 nn.init.constant_(m.weight, 1)\n",
    "# #                 nn.init.constant_(m.bias, 0)\n",
    "#     self.set_last_layer_incorrect_connection(incorrect_strength=-0.5)\n",
    "  \n",
    "  \n",
    "#   def push_forward(self, x,params):\n",
    "#     '''this method is needed for the pushing operation'''\n",
    "#     for block in [1, 2, 3,4]:\n",
    "#         x = ConvBlockFunction(\n",
    "#             x,\n",
    "#             params[f'conv{block}.0.weight'],\n",
    "#             params[f'conv{block}.0.bias'],\n",
    "#             params.get(f'conv{block}.1.weight'),\n",
    "#             params.get(f'conv{block}.1.bias'))\n",
    "#     # for block in [4]:\n",
    "#     #     x = ConvBlockFunction4(\n",
    "#     #         x,\n",
    "#     #         params[f'conv{block}.0.weight'],\n",
    "#     #         params[f'conv{block}.0.bias'],\n",
    "#     #         params.get(f'conv{block}.1.weight'),\n",
    "#     #         params.get(f'conv{block}.1.bias'))\n",
    "#     conv_output=x\n",
    "    \n",
    "#     x2 = x ** 2\n",
    "#     x2_patch_sum = F.conv2d(input=x2, weight=params['ones'])\n",
    "#     p2 = params['prototype_vectors'] ** 2\n",
    "#     p2 = torch.sum(p2, dim=(1, 2, 3))\n",
    "#     # p2 is a vector of shape (num_prototypes,)\n",
    "#     # then we reshape it to (num_prototypes, 1, 1)\n",
    "#     p2_reshape = p2.view(-1, 1, 1)\n",
    "\n",
    "#     xp = F.conv2d(input=x, weight=params['prototype_vectors'])\n",
    "#     intermediate_result = - 2 * xp + p2_reshape  # use broadcast\n",
    "#     # x2_patch_sum and intermediate_result are of the same shape\n",
    "#     distances = F.relu(x2_patch_sum + intermediate_result)\n",
    "#     return conv_output, distances\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5n1Ncark037l"
   },
   "outputs": [],
   "source": [
    "def create_label(n_way, k_shot):\n",
    "    return (torch.arange(n_way)\n",
    "                .repeat_interleave(k_shot)\n",
    "                .long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F-vfm7Xq037l"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, val_label,n_way,q_query):\n",
    "    \"\"\" utility function for accuracy calculation \"\"\"\n",
    "    acc = np.asarray([(\n",
    "          torch.argmax(logits, -1).cpu().numpy() == val_label.cpu().numpy())]\n",
    "          ).mean()\n",
    "    precision =[0]*n_way\n",
    "    recall = [0]*n_way\n",
    "    f1 = [0]*n_way\n",
    "    TFP=[0]*n_way\n",
    "    TP=[0]*n_way\n",
    "    pred = torch.argmax(logits, -1).cpu().numpy()\n",
    "    ground = val_label.cpu().numpy()\n",
    "    for index in range(len(ground)):\n",
    "        for n_index in range(n_way):\n",
    "            if pred[index]==n_index:\n",
    "                TFP[n_index]=TFP[n_index]+1\n",
    "                if ground[index]==n_index:\n",
    "                    TP[n_index] = TP[n_index]+1\n",
    "        for n_index in range(n_way):\n",
    "            if TP[n_index] == 0:\n",
    "                precision[n_index]=0\n",
    "                recall[n_index]=0\n",
    "                f1[n_index]=0\n",
    "            else:\n",
    "                precision[n_index] = TP[n_index]/TFP[n_index]\n",
    "                recall[n_index] = TP[n_index]/q_query\n",
    "                f1[n_index] = 2*precision[n_index]*recall[n_index]/(precision[n_index]+recall[n_index])\n",
    "\n",
    "\n",
    "    return acc,np.array(precision).mean(),np.array(recall).mean(),np.array(f1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LRJLdr26037m"
   },
   "outputs": [],
   "source": [
    "def predict_true(logits, val_label):\n",
    "    \"\"\" utility function for accuracy calculation \"\"\"\n",
    "    acc = np.asarray([(\n",
    "      torch.argmax(logits, -1).cpu().numpy() == val_label.cpu().numpy())]\n",
    "      )\n",
    "    acc= acc[0]\n",
    "    res=[]\n",
    "    for x in acc:\n",
    "        res =res+[bool(x)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xOhXdYiSiMSq"
   },
   "outputs": [],
   "source": [
    "class MiniImageNet(Dataset):\n",
    "    def __init__(self, data_dir,k_way,q_query):\n",
    "        \"\"\"Dataset class representing miniImageNet dataset\n",
    "\n",
    "        # Arguments:\n",
    "            subset: Whether the dataset represents the background or evaluation set\n",
    "        \"\"\"\n",
    "        self.file_list = [f for f in glob.glob(\n",
    "            data_dir+\"*\", \n",
    "            recursive=True)]\n",
    "        self.n = k_way + q_query        \n",
    "\n",
    "        normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                  std=(0.229, 0.224, 0.225))\n",
    "        # Setup transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = np.arange(600)\n",
    "\n",
    "        # For random sampling the characters we want.\n",
    "        np.random.shuffle(sample)\n",
    "        img_path = self.file_list[idx]\n",
    "        img_list = [f for f in glob.glob(\n",
    "            img_path + \"**/*.jpg\", recursive=True)]\n",
    "        img_list.sort()\n",
    "        imgs = [self.transform(\n",
    "            Image.open(img_file)) \n",
    "            for img_file in img_list]\n",
    "        # `k_way + q_query` examples for each character\n",
    "        imgs = torch.stack(imgs)[sample[:self.n]]\n",
    "        lst = [img_path.split('/')[-1]]+[imgs]\n",
    "        return lst\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2vsJ3GO037n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLdsKuMPY4wr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "p-3nphb8037n"
   },
   "outputs": [],
   "source": [
    "def inner_update_MAML(fast_weights, loss, inner_lr):\n",
    "    \"\"\" Inner Loop Update \"\"\"\n",
    "    grads = torch.autograd.grad(\n",
    "        loss, fast_weights.values(), create_graph=True)\n",
    "    # Perform SGD\n",
    "    fast_weights = OrderedDict(\n",
    "        (name, param - inner_lr * grad)\n",
    "        for ((name, param), grad) in zip(fast_weights.items(), grads))\n",
    "    return fast_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9YyeSzLo037o"
   },
   "outputs": [],
   "source": [
    "def collect_gradients_MAML(\n",
    "    special_grad: OrderedDict, fast_weights, model, len_data):\n",
    "    \"\"\" Actually do nothing (just backwards later) \"\"\"\n",
    "    return special_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mQ7A_25M037o"
   },
   "outputs": [],
   "source": [
    "def outer_update_MAML(model, meta_batch_loss, grad_tensors):\n",
    "    \"\"\" Simply backwards \"\"\"\n",
    "    meta_batch_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4AZNF4RT037o"
   },
   "outputs": [],
   "source": [
    "def dataloader_init(datasets,n_way,num_workers=4):\n",
    "    train_set, val_set, test_set = datasets\n",
    "    train_loader = DataLoader(train_set,\n",
    "                # The \"batch_size\" here is not \\\n",
    "                #    the meta batch size, but  \\\n",
    "                #    how many different        \\\n",
    "                #    characters in a task,     \\\n",
    "                #    i.e. the \"n_way\" in       \\\n",
    "                #    few-shot classification.\n",
    "                batch_size=n_way,\n",
    "                num_workers=num_workers,\n",
    "                shuffle=True,\n",
    "                drop_last=True)\n",
    "    val_loader = DataLoader(val_set,\n",
    "              batch_size=n_way,\n",
    "              num_workers=num_workers,\n",
    "              shuffle=True,\n",
    "              drop_last=True)\n",
    "    test_loader = DataLoader(test_set,\n",
    "              batch_size=n_way,\n",
    "              num_workers=num_workers,\n",
    "              shuffle=True,\n",
    "              drop_last=True)\n",
    "    train_iter = iter(train_loader)\n",
    "    val_iter = iter(val_loader)\n",
    "    test_iter = iter(test_loader)\n",
    "    return (train_loader, val_loader, test_loader), \\\n",
    "          (train_iter, val_iter, test_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OJfcC9-9037p"
   },
   "outputs": [],
   "source": [
    "def model_init(n_way,in_ch,meta_lr):\n",
    "    meta_model = Classifier(in_ch, n_way).to(device)\n",
    "    optimizer = torch.optim.Adam(meta_model.parameters(), \n",
    "                                lr=meta_lr)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    # return meta_model, False, loss_fn\n",
    "    return meta_model,optimizer,loss_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tzOtiiqY037p"
   },
   "outputs": [],
   "source": [
    "# def model_init_new(n_way,in_ch,meta_lr,prototype_shape,img_size,epsilon,prototype_activation_function):\n",
    "#   meta_multi = newClassifier(in_ch, n_way,prototype_shape,img_size,epsilon,prototype_activation_function).to(device)\n",
    "#   joint_optimizer_lrs = {'features': 1e-4,# 有改动，原来1e-4\n",
    "# #                        'add_on_layers': 3e-3,\n",
    "#                       'prototype_vectors':3e-3}\n",
    "#   joint_optimizer_specs = \\\n",
    "#   [\n",
    "#       {'params': meta_multi.features.parameters(), 'lr': joint_optimizer_lrs['features']}, # bias are now also being regularized\n",
    "#     # {'params': meta_multi.conv2.parameters(), 'lr': joint_optimizer_lrs['features']},\n",
    "#     # {'params': meta_multi.conv3.parameters(), 'lr': joint_optimizer_lrs['features']},\n",
    "#     # {'params': meta_multi.conv4.parameters(), 'lr': joint_optimizer_lrs['features']},\n",
    "#   #  {'params': meta_multi.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers']},\n",
    "#     {'params': meta_multi.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "#   # {'params': meta_multi.logits.parameters(), 'lr':1e-4}\n",
    "#   ]\n",
    "#   joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n",
    "#   loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "#   return meta_multi,joint_optimizer,loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nYarc7wD037q"
   },
   "outputs": [],
   "source": [
    "#helper.py\n",
    "def makedir(path):\n",
    "  '''\n",
    "  if path does not exist in the file system, create it\n",
    "  '''\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "\n",
    "def find_high_activation_crop(activation_map, percentile=60):\n",
    "    threshold = np.percentile(activation_map, percentile)\n",
    "    mask = np.ones(activation_map.shape)\n",
    "    mask[activation_map < threshold] = 0\n",
    "    lower_y, upper_y, lower_x, upper_x = 0, 0, 0, 0\n",
    "    for i in range(mask.shape[0]):\n",
    "        if np.amax(mask[i]) > 0.5:\n",
    "            lower_y = i\n",
    "            break\n",
    "    for i in reversed(range(mask.shape[0])):\n",
    "        if np.amax(mask[i]) > 0.5:\n",
    "            upper_y = i\n",
    "            break\n",
    "    for j in range(mask.shape[1]):\n",
    "        if np.amax(mask[:,j]) > 0.5:\n",
    "            lower_x = j\n",
    "            break\n",
    "    for j in reversed(range(mask.shape[1])):\n",
    "        if np.amax(mask[:,j]) > 0.5:\n",
    "            upper_x = j\n",
    "            break\n",
    "    return lower_y, upper_y+1, lower_x, upper_x+1\n",
    "\n",
    "def coumpute_zjym_prototype(size_ori,size_now,position):\n",
    "    patch=size_ori // size_now\n",
    "    fmap_height_start_index = position[1] * patch\n",
    "    fmap_height_end_index = (position[1]+1)*patch\n",
    "    fmap_width_start_index = position[2] * patch\n",
    "    fmap_width_end_index = (position[2]+1)*patch    \n",
    "    return [position[0],fmap_height_start_index,fmap_height_end_index,fmap_width_start_index,fmap_width_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jfh3vU5H037q"
   },
   "outputs": [],
   "source": [
    "# def SimilarityConsis(model,\n",
    "#         search_batch_input,\n",
    "#         prototype_network_parallel,\n",
    "#         fast_weights,\n",
    "#         n_way,q_query,\n",
    "#         search_y=None, # required if class_specific == True\n",
    "#         num_classes=None):\n",
    "\n",
    "#   search_batch = search_batch_input\n",
    "#   with torch.no_grad():\n",
    "#     search_batch = search_batch.cuda()\n",
    "#     # this computation currently is not parallelized\n",
    "#     _, proto_dist_torch = model.push_forward(search_batch,fast_weights)\n",
    "\n",
    "#   proto_dist_ = np.copy(proto_dist_torch.detach().cpu().numpy())\n",
    "\n",
    "#   del proto_dist_torch\n",
    "\n",
    "#   class_to_img_index_dict = {key: [] for key in range(n_way)}\n",
    "#       # img_y is the image's integer label\n",
    "#   for img_index, img_y in enumerate(search_y):\n",
    "#     img_label = img_y.item()\n",
    "#     class_to_img_index_dict[img_label].append(img_index)\n",
    "\n",
    "#   prototype_shape = model.prototype_shape\n",
    "#   n_prototypes = prototype_shape[0]\n",
    "#   x= list(np.zeros((q_query,n_prototypes)))\n",
    "#   for j in range(n_prototypes):\n",
    "#     target_class = torch.argmax(model.prototype_class_identity[j]).item()\n",
    "#     proto_dist_j = proto_dist_[class_to_img_index_dict[target_class]][:,j,:,:]# class_to_img_index_dict[target_class]为原型所属的类，[:,j,:,:]为第j个原型。意思是这个类下的图片和第j个原型的结果。\n",
    "#     for m in range(len(proto_dist_j)):\n",
    "#       quer_proto_dist_j = proto_dist_j[m]\n",
    "#       index=np.argmin(quer_proto_dist_j, axis=None)\n",
    "#       x[m][j]=int(index)\n",
    "#   res = 0\n",
    "#   for i in range(len(x)):\n",
    "#     if res ==0:\n",
    "#       res=[[]]\n",
    "#     else:\n",
    "#       res.append([[]])\n",
    "#     for j in range(len(x[0])):\n",
    "#       res[i]=res[i]+[int(x[i][j])]\n",
    "#   return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Nnx4SI1G037q"
   },
   "outputs": [],
   "source": [
    "# def update_prototypes_on_batch(search_batch_input,\n",
    "#                 start_index_of_search_batch,\n",
    "#                 prototype_network_parallel,\n",
    "#                 global_min_proto_dist, # this will be updated\n",
    "#                 global_min_fmap_patches, # this will be updated\n",
    "#                 proto_rf_boxes, # this will be updated\n",
    "#                 proto_bound_boxes, # this will be updated\n",
    "#                 fast_weights,\n",
    "#                 class_specific=True,\n",
    "#                 search_y=None, # required if class_specific == True\n",
    "#                 num_classes=None, # required if class_specific == True\n",
    "#                 prototype_layer_stride=1,\n",
    "#                 dir_for_saving_prototypes=None,\n",
    "#                 prototype_img_filename_prefix=None,\n",
    "#                 prototype_self_act_filename_prefix=None,\n",
    "#                 prototype_activation_function_in_numpy=None):\n",
    "\n",
    "# #     prototype_network_parallel.eval()\n",
    "\n",
    "\n",
    "#       # print('preprocessing input for pushing ...')\n",
    "#       # search_batch = copy.deepcopy(search_batch_input)\n",
    "#   # 可能有问题\n",
    "# #     search_batch = preprocess(search_batch_input)\n",
    "#   search_batch = search_batch_input\n",
    "#   with torch.no_grad():\n",
    "#     search_batch = search_batch.cuda()\n",
    "#     # this computation currently is not parallelized\n",
    "#     protoL_input_torch, proto_dist_torch = prototype_network_parallel.push_forward(search_batch,fast_weights)\n",
    "\n",
    "#   protoL_input_ = np.copy(protoL_input_torch.detach().cpu().numpy())\n",
    "#   proto_dist_ = np.copy(proto_dist_torch.detach().cpu().numpy())\n",
    "\n",
    "#   del protoL_input_torch, proto_dist_torch\n",
    "\n",
    "#   if class_specific:\n",
    "#     class_to_img_index_dict = {key: [] for key in range(num_classes)}\n",
    "#     # img_y is the image's integer label\n",
    "#     for img_index, img_y in enumerate(search_y):\n",
    "#       img_label = img_y.item()\n",
    "#       class_to_img_index_dict[img_label].append(img_index)\n",
    "\n",
    "#   prototype_shape = prototype_network_parallel.prototype_shape\n",
    "#   n_prototypes = prototype_shape[0]\n",
    "#   proto_h = prototype_shape[2]\n",
    "#   proto_w = prototype_shape[3]\n",
    "#   max_dist = prototype_shape[1] * prototype_shape[2] * prototype_shape[3]\n",
    "\n",
    "#   for j in range(n_prototypes):\n",
    "#     #if n_prototypes_per_class != None:\n",
    "#     if class_specific:\n",
    "#         # target_class is the class of the class_specific prototype\n",
    "#       target_class = torch.argmax(prototype_network_parallel.prototype_class_identity[j]).item()\n",
    "#       # if there is not images of the target_class from this batch\n",
    "#       # we go on to the next prototype\n",
    "#       if len(class_to_img_index_dict[target_class]) == 0:\n",
    "#         continue\n",
    "#       proto_dist_j = proto_dist_[class_to_img_index_dict[target_class]][:,j,:,:]# class_to_img_index_dict[target_class]为原型所属的类，[:,j,:,:]为第j个原型。意思是这个类下的图片和第j个原型的结果。\n",
    "#     else:\n",
    "#       # if it is not class specific, then we will search through\n",
    "#       # every example\n",
    "#       proto_dist_j = proto_dist_[:,j,:,:]\n",
    "\n",
    "#     batch_min_proto_dist_j = np.amin(proto_dist_j)# 第j个原型所属的第x类的最近的值。（激活最大的地方）\n",
    "#     if batch_min_proto_dist_j < global_min_proto_dist[j]:# \n",
    "#       batch_argmin_proto_dist_j = \\\n",
    "#           list(np.unravel_index(np.argmin(proto_dist_j, axis=None),\n",
    "#                                 proto_dist_j.shape))# batch最小原型距离的坐标。\n",
    "#       if class_specific:\n",
    "#         '''\n",
    "#         change the argmin index from the index among\n",
    "#         images of the target class to the index in the entire search\n",
    "#         batch\n",
    "#         '''\n",
    "#         # 因为都是五分类，这一步可以省略。\n",
    "#         batch_argmin_proto_dist_j[0] = class_to_img_index_dict[target_class][batch_argmin_proto_dist_j[0]]\n",
    "\n",
    "#       # retrieve the corresponding feature map patch\n",
    "#       img_index_in_batch = batch_argmin_proto_dist_j[0]\n",
    "#       fmap_height_start_index = batch_argmin_proto_dist_j[1] * prototype_layer_stride\n",
    "#       fmap_height_end_index = fmap_height_start_index + proto_h\n",
    "#       fmap_width_start_index = batch_argmin_proto_dist_j[2] * prototype_layer_stride\n",
    "#       fmap_width_end_index = fmap_width_start_index + proto_w\n",
    "#       # 对应的features(x),对应的位置。\n",
    "#       batch_min_fmap_patch_j = protoL_input_[img_index_in_batch,\n",
    "#                                               :,\n",
    "#                                               fmap_height_start_index:fmap_height_end_index,\n",
    "#                                               fmap_width_start_index:fmap_width_end_index]\n",
    "#       # 此处应该会修改，毕竟每个五分类任务的训练集只有一个。\n",
    "#       global_min_proto_dist[j] = batch_min_proto_dist_j\n",
    "#       global_min_fmap_patches[j] = batch_min_fmap_patch_j\n",
    "      \n",
    "#       # get the receptive field boundary of the image patch\n",
    "#       # that generates the representation\n",
    "# #             protoL_rf_info = prototype_network_parallel.proto_layer_rf_info\n",
    "# #             rf_prototype_j = compute_rf_prototype(search_batch.size(2), batch_argmin_proto_dist_j, protoL_rf_info)\n",
    "#       rf_prototype_j = coumpute_zjym_prototype(search_batch.size(2),protoL_input_.shape[2],batch_argmin_proto_dist_j)\n",
    "#       # get the whole image\n",
    "#       original_img_j = search_batch_input[rf_prototype_j[0]]\n",
    "#       original_img_j = original_img_j.data.cpu().numpy()\n",
    "#       original_img_j = np.transpose(original_img_j, (1, 2, 0))\n",
    "#       original_img_size = original_img_j.shape[0]#28,28,1\n",
    "      \n",
    "#       # crop out the receptive field\n",
    "#       rf_img_j = original_img_j[rf_prototype_j[1]:rf_prototype_j[2],\n",
    "#                                 rf_prototype_j[3]:rf_prototype_j[4], :]\n",
    "      \n",
    "#       # save the prototype receptive field information\n",
    "#       proto_rf_boxes[j, 0] = rf_prototype_j[0] + start_index_of_search_batch\n",
    "#       proto_rf_boxes[j, 1] = rf_prototype_j[1]\n",
    "#       proto_rf_boxes[j, 2] = rf_prototype_j[2]\n",
    "#       proto_rf_boxes[j, 3] = rf_prototype_j[3]\n",
    "#       proto_rf_boxes[j, 4] = rf_prototype_j[4]\n",
    "#       if proto_rf_boxes.shape[1] == 6 and search_y is not None:# 记录类别。\n",
    "#           proto_rf_boxes[j, 5] = search_y[rf_prototype_j[0]].item()\n",
    "\n",
    "#       # find the highly activated region of the original image\n",
    "#       proto_dist_img_j = proto_dist_[img_index_in_batch, j, :, :]\n",
    "#       if prototype_network_parallel.prototype_activation_function == 'log':\n",
    "#           proto_act_img_j = np.log((proto_dist_img_j + 1) / (proto_dist_img_j + prototype_network_parallel.epsilon))\n",
    "#       elif prototype_network_parallel.prototype_activation_function == 'linear':\n",
    "#           proto_act_img_j = max_dist - proto_dist_img_j\n",
    "#       else:\n",
    "#           proto_act_img_j = prototype_activation_function_in_numpy(proto_dist_img_j)\n",
    "#       upsampled_act_img_j = cv2.resize(proto_act_img_j, dsize=(original_img_size, original_img_size),\n",
    "#                                         interpolation=cv2.INTER_CUBIC)\n",
    "#       proto_bound_j = find_high_activation_crop(upsampled_act_img_j)\n",
    "#       # crop out the image patch with high activation as prototype image\n",
    "#       proto_img_j = original_img_j[proto_bound_j[0]:proto_bound_j[1],\n",
    "#                                     proto_bound_j[2]:proto_bound_j[3], :]\n",
    "\n",
    "#       # save the prototype boundary (rectangular boundary of highly activated region)\n",
    "#       proto_bound_boxes[j, 0] = proto_rf_boxes[j, 0]\n",
    "#       proto_bound_boxes[j, 1] = proto_bound_j[0]\n",
    "#       proto_bound_boxes[j, 2] = proto_bound_j[1]\n",
    "#       proto_bound_boxes[j, 3] = proto_bound_j[2]\n",
    "#       proto_bound_boxes[j, 4] = proto_bound_j[3]\n",
    "#       if proto_bound_boxes.shape[1] == 6 and search_y is not None:\n",
    "#         proto_bound_boxes[j, 5] = search_y[rf_prototype_j[0]].item()\n",
    "\n",
    "#       if dir_for_saving_prototypes is not None:\n",
    "#         if prototype_self_act_filename_prefix is not None:\n",
    "#           # save the numpy array of the prototype self activation\n",
    "#           np.save(os.path.join(dir_for_saving_prototypes,\n",
    "#                     prototype_self_act_filename_prefix + str(j) + '.npy'),\n",
    "#                     proto_act_img_j)\n",
    "#         if prototype_img_filename_prefix is not None:\n",
    "#             # save the whole image containing the prototype as png\n",
    "# #                     plusBB(original_img_j,proto_bound_j[0],proto_bound_j[1],proto_bound_j[2],proto_bound_j[3],os.path.join(dir_for_saving_prototypes,\n",
    "# #                                             prototype_img_filename_prefix + '-original' + str(j) + '.png'))\n",
    "# #                     cv2.rectangle(original_img_j,(int(proto_bound_j[0]),int(proto_bound_j[1])),(int(proto_bound_j[2]),int(proto_bound_j[3])),(0,255,0),3)  \n",
    "#           plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "#                                   prototype_img_filename_prefix + '-original' + str(j) + '.png'),\n",
    "#                       original_img_j,\n",
    "# #                                np.squeeze(original_img_j),\n",
    "#                       vmin=0.0,\n",
    "#                       vmax=1.0)\n",
    "#           # overlay (upsampled) self activation on original image and save the result\n",
    "#           rescaled_act_img_j = upsampled_act_img_j - np.amin(upsampled_act_img_j)\n",
    "#           rescaled_act_img_j = rescaled_act_img_j / np.amax(rescaled_act_img_j)\n",
    "#           heatmap = cv2.applyColorMap(np.uint8(255*rescaled_act_img_j), cv2.COLORMAP_JET)\n",
    "#           heatmap = np.float32(heatmap) / 255\n",
    "#           heatmap = heatmap[...,::-1]\n",
    "#           overlayed_original_img_j = 0.5 * original_img_j + 0.3 * heatmap\n",
    "#           plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "#                                   prototype_img_filename_prefix + '-original_with_self_act' + str(j) + '.png'),\n",
    "#                       overlayed_original_img_j,\n",
    "#                       vmin=0.0,\n",
    "#                       vmax=1.0)\n",
    "          \n",
    "#           # if different from the original (whole) image, save the prototype receptive field as png\n",
    "#           if rf_img_j.shape[0] != original_img_size or rf_img_j.shape[1] != original_img_size:\n",
    "#             plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "#                                     prototype_img_filename_prefix + '-receptive_field' + str(j) + '.png'),\n",
    "# #                                    np.squeeze(rf_img_j),\n",
    "#                         rf_img_j,\n",
    "#                         vmin=0.0,\n",
    "#                         vmax=1.0)\n",
    "#             overlayed_rf_img_j = overlayed_original_img_j[rf_prototype_j[1]:rf_prototype_j[2],\n",
    "#                                                           rf_prototype_j[3]:rf_prototype_j[4]]\n",
    "#             plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "#                                     prototype_img_filename_prefix + '-receptive_field_with_self_act' + str(j) + '.png'),\n",
    "#                         overlayed_rf_img_j,\n",
    "#                         vmin=0.0,\n",
    "#                         vmax=1.0)\n",
    "          \n",
    "#           # save the prototype image (highly activated region of the whole image)\n",
    "#           plt.imsave(os.path.join(dir_for_saving_prototypes,\n",
    "#                 prototype_img_filename_prefix + str(j) + '.png'),\n",
    "# #                                 np.squeeze(proto_img_j),\n",
    "#                   proto_img_j,\n",
    "#                   vmin=0.0,\n",
    "#                   vmax=1.0)\n",
    "          \n",
    "#   if class_specific:\n",
    "#       del class_to_img_index_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "m9eP0uWE037r"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def toList(tensor):\n",
    "    res = np.copy(tensor.detach().cpu().numpy())\n",
    "    return res.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "doi2fida037r"
   },
   "outputs": [],
   "source": [
    "def get_meta_batch(meta_batch_size,\n",
    "                   k_shot, q_query, \n",
    "                   data_loader, iterator):\n",
    "    data = []\n",
    "    label_lst =[]\n",
    "    for _ in range(meta_batch_size):\n",
    "        try:\n",
    "            # a \"task_data\" tensor is representing \\\n",
    "            #     the data of a task, with size of \\\n",
    "            #     [n_way, k_shot+q_query, 1, 28, 28]\n",
    "            task_data = iterator.next()  \n",
    "        except StopIteration:\n",
    "            iterator = iter(data_loader)\n",
    "            task_data = iterator.next()\n",
    "        label = task_data[0]\n",
    "        label_lst.append(label)\n",
    "        task_data = task_data[1]\n",
    "        train_data = (task_data[:, :k_shot]\n",
    "                      .reshape(-1, 3, 224, 224))\n",
    "        val_data = (task_data[:, k_shot:]\n",
    "                    .reshape(-1, 3,224, 224))\n",
    "        task_data = torch.cat(\n",
    "            (train_data, val_data), 0)\n",
    "        data.append(task_data)\n",
    "    return torch.stack(data).to(device), iterator,label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "l65DiZVwYOrY"
   },
   "outputs": [],
   "source": [
    "def labeltoindex(label):\n",
    "    with open('/content/imagenet_index_class.json', 'r', encoding='utf8')as fp:\n",
    "        labels_all = json.load(fp)\n",
    "    index = labels_all[label]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "sQbzSNq3YOtl"
   },
   "outputs": [],
   "source": [
    "def find_label(label_lst,k_shot):\n",
    "    labels = list(label_lst)\n",
    "    index_lst=[]\n",
    "    for label in labels:\n",
    "        index = labeltoindex(label)\n",
    "        index_lst=index_lst+[index]*k_shot\n",
    "    # index_lst.append(labeltoindex(label))\n",
    "    return torch.tensor(index_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "RNt3DY2c037s"
   },
   "outputs": [],
   "source": [
    "def MetaAlgorithmGenerator(\n",
    "    inner_update      = inner_update_MAML, \n",
    "    collect_gradients = collect_gradients_MAML, \n",
    "    outer_update      = outer_update_MAML):\n",
    "\n",
    "    global calculate_accuracy\n",
    "\n",
    "    def MetaAlgorithm(\n",
    "        model, optimizer, x, n_way, k_shot, q_query, loss_fn,label_lst,\n",
    "        inner_train_step=5, inner_lr=0.01, train=True): \n",
    "        criterion = loss_fn\n",
    "        task_loss, task_acc,task_pre,task_recall,task_f1 = [], [],[],[],[]\n",
    "        special_grad = OrderedDict()  # Added for variants!\n",
    "#         coefs = {\n",
    "#             'crs_ent': 1,\n",
    "#             'clst': 0.8,\n",
    "#             'sep': -0.08,\n",
    "#             'l1': 1e-4,\n",
    "#             }\n",
    "        for task_index,meta_batch in enumerate(x):\n",
    "            support_set = meta_batch[: n_way * k_shot]  \n",
    "            query_set = meta_batch[n_way * k_shot :]    \n",
    "\n",
    "            fast_weights = OrderedDict(model.named_parameters())\n",
    "\n",
    "          ### ---------- INNER TRAIN LOOP ---------- ###\n",
    "            for inner_step in range(inner_train_step): \n",
    "                train_label = create_label(n_way, k_shot).to(device)\n",
    "                logits = model.functional_forward(support_set, fast_weights)\n",
    "                loss = criterion(logits, train_label)\n",
    "                # logits = predict_conv(support_set)\n",
    "                # loss = criterion(logits, train_label)\n",
    "                fast_weights = inner_update(fast_weights, loss, inner_lr)\n",
    "\n",
    "          ### ---------- INNER VALID LOOP ---------- ###\n",
    "          # val_label = create_label(label_lst[task_index], q_query).to(device)\n",
    "            val_label=create_label(n_way, k_shot).to(device)\n",
    "          # FIXME: W for val?\n",
    "          # special_grad = collect_gradients(\n",
    "          #     special_grad, fast_weights, model, len(x))\n",
    "\n",
    "          # Collect gradients for outer loop\n",
    "            logits = model.functional_forward(query_set, fast_weights)\n",
    "          # logits = model.functional_forward(query_set, fast_weights) \n",
    "            loss = criterion(logits, val_label)\n",
    "            task_loss.append(loss)\n",
    "            acc,precision,recall,f1 = calculate_accuracy(logits, val_label,n_way,q_query)\n",
    "            task_acc.append(acc)\n",
    "            task_pre.append(precision)\n",
    "            task_recall.append(recall)\n",
    "            task_f1.append(f1)\n",
    "        # Update outer loop\n",
    "        # model.train()\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        meta_batch_loss = torch.stack(task_loss).mean()\n",
    "        if train:\n",
    "            # Notice the update part!\n",
    "            outer_update(model, meta_batch_loss, special_grad)\n",
    "            optimizer.step()\n",
    "        task_acc = np.mean(task_acc)\n",
    "        task_pre=np.mean(task_pre)\n",
    "        task_recall=np.mean(task_recall)\n",
    "        task_f1 = np.mean(task_f1)\n",
    "        return meta_batch_loss, task_acc,task_pre,task_recall,task_f1\n",
    "    return MetaAlgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "plbEDQqM037s"
   },
   "outputs": [],
   "source": [
    "# def MetaAlgorithmGeneratorNew(\n",
    "#   inner_update      = inner_update_MAML, \n",
    "#   collect_gradients = collect_gradients_MAML, \n",
    "#   outer_update      = outer_update_MAML):\n",
    "\n",
    "#   global calculate_accuracy\n",
    "\n",
    "#   def MetaAlgorithmNew(\n",
    "#     model, optimizer, x, n_way, k_shot, q_query, loss_fn,epoch_number,step,\n",
    "#     inner_train_step=10, inner_lr=0.04, train=0): \n",
    "#     criterion = loss_fn\n",
    "#     task_loss, task_acc = [], []\n",
    "#     special_grad = OrderedDict()  # Added for variants!\n",
    "#     coefs = {\n",
    "#         'crs_ent': 1,\n",
    "#         'clst': 0.8,\n",
    "#         'sep': -0.8,\n",
    "#         'l1': 1e-4,\n",
    "#         }\n",
    "#     task_crsEnt,task_cls,task_sep,task_l1 = [],[],[],[]\n",
    "#     task_dic={}\n",
    "#     task_dic_path='./test'\n",
    "#     makedir(task_dic_path)\n",
    "#     for task_index,meta_batch in enumerate(x):\n",
    "# #             print(task_index)\n",
    "#       support_set = meta_batch[: n_way * k_shot]  \n",
    "#       query_set = meta_batch[n_way * k_shot :]    \n",
    "      \n",
    "#       fast_weights = OrderedDict(model.named_parameters())\n",
    "      \n",
    "#       ### ---------- INNER TRAIN LOOP ---------- ###\n",
    "#       for inner_step in range(inner_train_step): \n",
    "#         train_label = create_label(n_way, k_shot).to(device)\n",
    "#         logits,min_distances = model.functional_forward(support_set, fast_weights)\n",
    "#         cross_entropy = criterion(logits, train_label)\n",
    "\n",
    "#         max_dist = (model.prototype_shape[1]\n",
    "#                     * model.prototype_shape[2]\n",
    "#                     * model.prototype_shape[3])\n",
    "#         # calculate cluster cost                \n",
    "#         prototypes_of_correct_class = torch.t(model.prototype_class_identity[:,train_label]).cuda()\n",
    "#         inverted_distances, _ = torch.max((max_dist - min_distances) * prototypes_of_correct_class, dim=1)\n",
    "#         cluster_cost = torch.mean(max_dist - inverted_distances)\n",
    "        \n",
    "        \n",
    "#         # calculate separation cost\n",
    "#         prototypes_of_wrong_class = 1 - prototypes_of_correct_class\n",
    "#         inverted_distances_to_nontarget_prototypes, _ = \\\n",
    "#             torch.max((max_dist - min_distances) * prototypes_of_wrong_class, dim=1)\n",
    "#         separation_cost = torch.mean(max_dist - inverted_distances_to_nontarget_prototypes)\n",
    "            \n",
    "#         # calculate avg cluster cost\n",
    "#         avg_separation_cost = \\\n",
    "#             torch.sum(min_distances * prototypes_of_wrong_class, dim=1) / torch.sum(prototypes_of_wrong_class, dim=1)\n",
    "#         avg_separation_cost = torch.mean(avg_separation_cost)\n",
    "        \n",
    "        \n",
    "#         l1_mask = 1 - torch.t(model.prototype_class_identity).cuda()\n",
    "#         l1 = (model.logits.weight * l1_mask).norm(p=1)\n",
    "#         loss = (coefs['crs_ent'] * cross_entropy\n",
    "#                           + coefs['clst'] * cluster_cost\n",
    "#                           + coefs['sep'] * separation_cost\n",
    "#                           + coefs['l1'] * l1)\n",
    "#         ones =  fast_weights.pop('ones')\n",
    "#         logitstemp = fast_weights.pop('logits.weight')\n",
    "#         fast_weights = inner_update(fast_weights, loss, inner_lr)\n",
    "        \n",
    "#         fast_weights['ones'] = ones\n",
    "#         fast_weights['logits.weight'] = logitstemp\n",
    "      \n",
    "#       # push prototype\n",
    "#       if epoch_number>27:\n",
    "#         train_label = create_label(n_way,k_shot).to(device)\n",
    "\n",
    "#         prototype_shape = model.prototype_shape\n",
    "#         n_prototypes = model.num_prototypes\n",
    "#         global_min_proto_dist = np.full(n_prototypes, np.inf)\n",
    "#         global_min_fmap_patches = np.zeros(\n",
    "#             [n_prototypes,\n",
    "#               prototype_shape[1],\n",
    "#               prototype_shape[2],\n",
    "#               prototype_shape[3]])\n",
    "#         proto_rf_boxes = np.full(shape=[n_prototypes, 6],\n",
    "#                             fill_value=-1)\n",
    "#         proto_bound_boxes = np.full(shape=[n_prototypes, 6],\n",
    "#                                     fill_value=-1)\n",
    "#         makedir('./img')\n",
    "#         root_dir_for_saving_prototypes = os.path.join('./img', 'epoch-'+str(epoch_number))\n",
    "#         task_dic_path = os.path.join(task_dic_path,'epoch-'+str(epoch_number))\n",
    "#         if train==0:\n",
    "#           root_dir_for_saving_prototypes = os.path.join(root_dir_for_saving_prototypes, 'train')\n",
    "#         elif train==1:\n",
    "#           root_dir_for_saving_prototypes = os.path.join(root_dir_for_saving_prototypes, 'val')\n",
    "#         else:\n",
    "#           root_dir_for_saving_prototypes = os.path.join('./test','epoch-'+str(epoch_number))\n",
    "#           root_dir_for_saving_prototypes =os.path.join(root_dir_for_saving_prototypes,'prototype')\n",
    "#         root_dir_for_saving_prototypes = os.path.join(root_dir_for_saving_prototypes,'step-'+str(step))\n",
    "        \n",
    "#         makedir(root_dir_for_saving_prototypes)\n",
    "#         # 此处的epoch_number可能要传epoch进来\n",
    "# #                 print(os.path.join(root_dir_for_saving_prototypes,'task-'+str(task_index)))\n",
    "#         proto_epoch_dir =os.path.join(root_dir_for_saving_prototypes,'task-'+str(task_index))\n",
    "        \n",
    "#         makedir(proto_epoch_dir)\n",
    "#         search_batch_size =n_way\n",
    "\n",
    "#         num_classes = model.num_classes\n",
    "# #             for push_iter, search_batch_inpu in eupdate_prototypes_on_batchnumerate(support_set):\n",
    "\n",
    "#         start_index_of_search_batch = 0\n",
    "\n",
    "#         update_prototypes_on_batch(support_set,\n",
    "#                             start_index_of_search_batch,\n",
    "#                             model,\n",
    "#                             global_min_proto_dist,\n",
    "#                             global_min_fmap_patches,\n",
    "#                             proto_rf_boxes,\n",
    "#                             proto_bound_boxes,\n",
    "#                             fast_weights,\n",
    "#                             class_specific=True,\n",
    "#                             search_y=train_label,\n",
    "#                             num_classes=num_classes,\n",
    "#                             prototype_layer_stride=1,\n",
    "#                             dir_for_saving_prototypes=proto_epoch_dir,\n",
    "#                             prototype_img_filename_prefix='prototype-img',\n",
    "#                             prototype_self_act_filename_prefix='prototype-self-act')    \n",
    "\n",
    "#         proto_bound_boxes_filename_prefix='bb'\n",
    "#         # proto_bound_boxes_filename_prefix ='bb'=='None'\n",
    "#         if proto_epoch_dir != None and proto_bound_boxes_filename_prefix != None:\n",
    "#           np.save(os.path.join(proto_epoch_dir, proto_bound_boxes_filename_prefix + str(epoch_number) + '.npy'),\n",
    "#                     proto_bound_boxes)     \n",
    "\n",
    "#         prototype_update = np.reshape(global_min_fmap_patches,\n",
    "#                               tuple(prototype_shape))\n",
    "        \n",
    "#         fast_weights['prototype_vectors'] = torch.tensor(prototype_update, dtype=torch.float32).cuda()\n",
    "      \n",
    "      \n",
    "      \n",
    "#       ### ---------- INNER VALID LOOP ---------- ###\n",
    "#       # ----inner test-----#\n",
    "#       val_label = create_label(n_way, q_query).to(device)\n",
    "#       # FIXME: W for val?\n",
    "#       ones =  fast_weights.pop('ones')            \n",
    "#       logitstemp = fast_weights.pop('logits.weight')\n",
    "      \n",
    "#       special_grad = collect_gradients(\n",
    "#           special_grad, fast_weights, model, len(x))\n",
    "      \n",
    "#       fast_weights['ones'] = ones\n",
    "#       fast_weights['logits.weight'] = logitstemp\n",
    "#       # Collect gradients for outer loop\n",
    "#       logits,min_distances = model.functional_forward(query_set, fast_weights) \n",
    "      \n",
    "#       cross_entropy = criterion(logits, val_label)\n",
    "#       all_dic={}\n",
    "#       index_dic={}\n",
    "#       high_dic={}\n",
    "#       if train ==2:\n",
    "#         if epoch_number>27:\n",
    "#           task_dic_path = os.path.join('./test','epoch-'+str(epoch_number))\n",
    "#           task_dic_path = os.path.join(task_dic_path,'original')\n",
    "#           task_dic_path = os.path.join(task_dic_path,'step-'+str(step))\n",
    "#           task_dic_path = os.path.join(task_dic_path,'task-'+str(task_index))\n",
    "#           makedir(task_dic_path)\n",
    "# #                 pp_distance=min_distances.cpu().numpy()\n",
    "#           for t in range(len(val_label)):\n",
    "#             original_img_j = query_set[t]\n",
    "#             original_img_j = original_img_j.data.cpu().numpy()\n",
    "#             original_img_j = np.transpose(original_img_j, (1, 2, 0))\n",
    "#             original_img_size = original_img_j.shape[0]\n",
    "#             if t % q_query ==0:\n",
    "#                 final_path = os.path.join(task_dic_path,'class-'+str(t//q_query))\n",
    "#                 makedir(final_path)\n",
    "                \n",
    "#             plt.imsave(os.path.join(final_path,str(t%q_query)+'.png'),\n",
    "#                     original_img_j,\n",
    "#                     vmin=0.0,\n",
    "#                     vmax=1.0)\n",
    "              \n",
    "#           task_dic['task-'+str(task_index)]=toList(min_distances)\n",
    "#           all_dic['res']=predict_true(logits,val_label)\n",
    "#           high_index= SimilarityConsis(model,query_set,model,fast_weights,n_way,q_query,search_y=val_label,num_classes=num_classes)\n",
    "#           index_dic['index']=high_index\n",
    "#           high_score =  torch.log((min_distances + 1) / (min_distances + model.epsilon))\n",
    "#           high_dic['res']=toList(high_score)\n",
    "#           jsondata = json.dumps(task_dic,indent=4)\n",
    "#           jsondata2 = json.dumps(all_dic,indent=4)\n",
    "#           jsondata3 = json.dumps(index_dic,indent=4)\n",
    "#           jsondata4 = json.dumps(high_dic,indent=4)\n",
    "#           f = open(task_dic_path+'/distance.json', 'w')\n",
    "#           f2= open(task_dic_path+'/predict_res.json','w')\n",
    "#           f3 =open(task_dic_path+'/index.json','w')\n",
    "#           f4=open(task_dic_path+'/activation.json','w')\n",
    "#           f.write(jsondata)\n",
    "#           f2.write(jsondata2)\n",
    "#           f3.write(jsondata3)\n",
    "#           f4.write(jsondata4)\n",
    "#           f.close()\n",
    "#           f2.close()\n",
    "#           f3.close()\n",
    "#           f4.close()\n",
    "#       max_dist = (model.prototype_shape[1]\n",
    "#           * model.prototype_shape[2]\n",
    "#           * model.prototype_shape[3])\n",
    "      \n",
    "#       # calculate cluster cost                \n",
    "#       prototypes_of_correct_class = torch.t(model.prototype_class_identity[:,val_label]).cuda()\n",
    "#       inverted_distances, _ = torch.max((max_dist - min_distances) * prototypes_of_correct_class, dim=1)\n",
    "#       cluster_cost = torch.mean(max_dist - inverted_distances)\n",
    "\n",
    "\n",
    "#       # calculate separation cost\n",
    "#       prototypes_of_wrong_class = 1 - prototypes_of_correct_class\n",
    "#       inverted_distances_to_nontarget_prototypes, _ = \\\n",
    "#           torch.max((max_dist - min_distances) * prototypes_of_wrong_class, dim=1)\n",
    "#       separation_cost = torch.mean(max_dist - inverted_distances_to_nontarget_prototypes)\n",
    "\n",
    "#       # calculate avg cluster cost\n",
    "#       avg_separation_cost = \\\n",
    "#           torch.sum(min_distances * prototypes_of_wrong_class, dim=1) / torch.sum(prototypes_of_wrong_class, dim=1)\n",
    "#       avg_separation_cost = torch.mean(avg_separation_cost)\n",
    "#       l1_mask = 1 - torch.t(model.prototype_class_identity).cuda()\n",
    "#       l1 = (model.logits.weight * l1_mask).norm(p=1)\n",
    "      \n",
    "#       loss = (coefs['crs_ent'] * cross_entropy\n",
    "#                         + coefs['clst'] * cluster_cost\n",
    "#                         + coefs['sep'] * separation_cost\n",
    "#                         + coefs['l1'] * l1)\n",
    "      \n",
    "#       task_loss.append(loss)\n",
    "#       task_acc.append(calculate_accuracy(logits, val_label))\n",
    "#       task_crsEnt.append(cross_entropy)\n",
    "#       task_cls.append(cluster_cost)\n",
    "#       task_sep.append(separation_cost)\n",
    "#       task_l1.append(l1)\n",
    "# #             task_acc.append(calculate_accuracy(logits, val_label))\n",
    "#     # Update outer loop\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     meta_batch_loss = torch.stack(task_loss).mean()\n",
    "#     meta_batch_crsEnt=torch.stack(task_crsEnt).mean()\n",
    "#     meta_batch_cls =torch.stack(task_cls).mean()\n",
    "#     meta_batch_sep = torch.stack(task_sep).mean()\n",
    "#     meta_batch_l1 = torch.stack(task_l1).mean()\n",
    "#     if train==0:\n",
    "#       ones =  fast_weights.pop('ones')\n",
    "#       logitstemp = fast_weights.pop('logits.weight')\n",
    "#       # Notice the update part!\n",
    "#       outer_update(model, meta_batch_loss, special_grad)\n",
    "#       fast_weights['ones'] = ones\n",
    "#       fast_weights['logits.weight'] = logitstemp            \n",
    "#       optimizer.step()\n",
    "#     task_acc = np.mean(task_acc)\n",
    "#     return meta_batch_loss, task_acc,meta_batch_crsEnt,meta_batch_cls,meta_batch_sep,meta_batch_l1\n",
    "#   return MetaAlgorithmNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VOG7gUiP037x"
   },
   "outputs": [],
   "source": [
    "def ori_outer_train_val(max_epoch\n",
    "            ,meta_model\n",
    "            ,MetaAlgorithm\n",
    "            ,optimizer\n",
    "            ,loss_fn\n",
    "            ,train_loader,val_loader\n",
    "            ,train_iter,val_iter\n",
    "            ,meta_batch_size,eval_batches\n",
    "            ,n_way\n",
    "            ,k_shot\n",
    "            ,q_query\n",
    "            ,inner_train_step=10\n",
    "            ,train=False):\n",
    "    for epoch in range(max_epoch):\n",
    "        print(\"Epoch %d\" % (epoch + 1))\n",
    "        train_meta_loss = []\n",
    "        train_acc = []\n",
    "        train_pre,train_recall,train_f1=[],[],[]\n",
    "        # The \"step\" here is a meta-gradinet update step\n",
    "        for step in tqdm(range(5)):\n",
    "                # len(train_loader) // meta_batch_size)): \n",
    "            x, train_iter,label_lst = get_meta_batch(\n",
    "              meta_batch_size, k_shot, q_query, \n",
    "              train_loader, train_iter)\n",
    "            meta_loss, acc,precision,recall,f1= MetaAlgorithm(\n",
    "              meta_model, optimizer, x, \n",
    "              n_way, k_shot, q_query, loss_fn,label_lst)\n",
    "            train_meta_loss.append(meta_loss.item())\n",
    "            train_acc.append(acc)\n",
    "            train_pre.append(precision)\n",
    "            train_recall.append(recall)\n",
    "            train_f1.append(f1)\n",
    "        print(\"  Loss    : \", \"%.3f\" % (np.mean(train_meta_loss)), end='\\t')\n",
    "        print(\"  Accuracy: \", \"%.3f %%\" % (np.mean(train_acc) * 100))\n",
    "        print(\"  Precision: \", \"%.3f %%\" % (np.mean(train_pre) * 100),end='\\t')\n",
    "        print(\"  Recall: \", \"%.3f %%\" % (np.mean(train_recall) * 100),end='\\t')\n",
    "        print(\"  F1: \", \"%.3f %%\" % (np.mean(train_f1) * 100),end='\\t')\n",
    "\n",
    "        # See the validation accuracy after each epoch.\n",
    "        # Early stopping is welcomed to implement.\n",
    "        val_acc = []\n",
    "        val_pre,val_recall,val_f1=[],[],[]\n",
    "        for eval_step in tqdm(range(6)):\n",
    "                # len(val_loader) // (eval_batches))):\n",
    "            x, val_iter,label_lst = get_meta_batch(\n",
    "              eval_batches, k_shot, q_query, \n",
    "              val_loader, val_iter)\n",
    "          # We update three inner steps when testing.\n",
    "            _, acc,precision,recall,f1 = MetaAlgorithm(meta_model, optimizer, x, \n",
    "                        n_way, k_shot, q_query, \n",
    "                        loss_fn, label_lst,\n",
    "                        inner_train_step=inner_train_step, \n",
    "                        train=train) \n",
    "            val_acc.append(acc)\n",
    "            val_pre.append(precision)\n",
    "            val_recall.append(recall)\n",
    "            val_f1.append(f1)\n",
    "        print(\"  Validation accuracy: \", \"%.3f %%\" % (np.mean(val_acc) * 100))\n",
    "        print(\"  Validation precision: \", \"%.3f %%\" % (np.mean(val_pre) * 100))\n",
    "        print(\"  Validation recall: \", \"%.3f %%\" % (np.mean(val_recall) * 100))\n",
    "        print(\"  Validation f1: \", \"%.3f %%\" % (np.mean(val_f1) * 100))\n",
    "    #         if np.mean(val_acc)>0.985:\n",
    "    #             break\n",
    "    return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "JfPMipr-037x"
   },
   "outputs": [],
   "source": [
    "def ori_outer_test(meta_model,MetaAlgorithm,optimizer,loss_fn,test_loader,test_iter,test_batches,n_way,k_shot,q_query,inner_train_step=10,train=False):\n",
    "    test_acc = []\n",
    "    test_pre,test_recall,test_f1=[],[],[]\n",
    "    for test_step in tqdm(range(6)):\n",
    "          # len(test_loader) // (test_batches))):\n",
    "        x, test_iter,label_lst = get_meta_batch(\n",
    "            test_batches, k_shot, q_query, \n",
    "            test_loader, test_iter)\n",
    "    # When testing, we update 3 inner-steps\n",
    "        _, acc,precision,recall,f1 = MetaAlgorithm(meta_model, optimizer, x, \n",
    "                  n_way, k_shot, q_query, loss_fn, label_lst,\n",
    "                  inner_train_step=inner_train_step, train=False)\n",
    "        test_acc.append(acc)\n",
    "        test_pre.append(precision)\n",
    "        test_recall.append(recall)\n",
    "        test_f1.append(f1)\n",
    "    print(\"  Testing accuracy: \", \"%.3f %%\" % (np.mean(test_acc) * 100))\n",
    "    print(\"  Testing precision: \", \"%.3f %%\" % (np.mean(test_pre) * 100))\n",
    "    print(\"  Testing recall: \", \"%.3f %%\" % (np.mean(test_recall) * 100))\n",
    "    print(\"  Testing f1: \", \"%.3f %%\" % (np.mean(test_f1) * 100))\n",
    "    return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Dj6rhZ4Y037y"
   },
   "outputs": [],
   "source": [
    "def ori_outer(n_way,k_shot,q_query,in_ch,meta_lr,max_epoch,train_loader,val_loader,test_loader,train_iter,val_iter,test_iter\n",
    "            ,meta_batch_size,eval_batches,test_batches,inner_train_step=10,train=False,model_ori_name=False,optimizer=False):\n",
    "    if optimizer != False:\n",
    "        print(\"1111\")\n",
    "        meta_model,_,loss_fn=model_init(n_way,in_ch,meta_lr)\n",
    "    else:\n",
    "        meta_model, optimizer, loss_fn = model_init(n_way,in_ch,meta_lr)\n",
    "    MetaAlgorithm = MetaAlgorithmGenerator()\n",
    "    try:\n",
    "        meta_model = torch.load(model_ori_name)\n",
    "    except:\n",
    "        print(\"can't find original model's name, please check 'model_ori_name'\")\n",
    "    meta_model=ori_outer_train_val(max_epoch\n",
    "                  ,meta_model\n",
    "                  ,MetaAlgorithm\n",
    "                  ,optimizer\n",
    "                  ,loss_fn\n",
    "                  ,train_loader,val_loader\n",
    "                  ,train_iter,val_iter\n",
    "                  ,meta_batch_size,eval_batches\n",
    "                  ,n_way\n",
    "                  ,k_shot\n",
    "                  ,q_query\n",
    "                  ,inner_train_step=inner_train_step\n",
    "                  ,train=train)\n",
    "    meta_model=ori_outer_test(meta_model\n",
    "                ,MetaAlgorithm\n",
    "                ,optimizer\n",
    "                ,loss_fn\n",
    "                ,test_loader\n",
    "                ,test_iter\n",
    "                ,test_batches\n",
    "                ,n_way\n",
    "                ,k_shot\n",
    "                ,q_query\n",
    "                ,inner_train_step=inner_train_step\n",
    "                ,train=train)\n",
    "  \n",
    "    return meta_model,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NNY5tDxx037y"
   },
   "outputs": [],
   "source": [
    "# def new_outer_train_val(max_epoch\n",
    "#               ,meta_model\n",
    "#               ,MetaAlgorithm_new\n",
    "#               ,optimizer\n",
    "#               ,loss_fn\n",
    "#               ,train_loader,val_loader\n",
    "#               ,train_iter,val_iter\n",
    "#               ,meta_batch_size,eval_batches\n",
    "#               ,n_way\n",
    "#               ,k_shot\n",
    "#               ,q_query\n",
    "#               ,inner_train_step=15\n",
    "#               ,train=1):\n",
    "#     for epoch in range(max_epoch):\n",
    "#         print(\"Epoch %d\" % (epoch + 1))\n",
    "#         train_meta_loss = []\n",
    "#         train_acc = []\n",
    "#         train_meta_crsEnt,train_meta_cls,train_meta_sep,train_meta_l1=[],[],[],[]\n",
    "#     # The \"step\" here is a meta-gradinet update step\n",
    "#         for step in tqdm(range(5)):\n",
    "#             # len(train_loader) // meta_batch_size)): \n",
    "#             x, train_iter,label_lst = get_meta_batch(\n",
    "#                 meta_batch_size, k_shot, q_query, \n",
    "#                 train_loader, train_iter)\n",
    "#             meta_loss, acc= MetaAlgorithm(\n",
    "#               meta_model, optimizer, x, \n",
    "#               n_way, k_shot, q_query, loss_fn,label_lst,epoch,step)\n",
    "#             train_meta_loss.append(meta_loss.item())\n",
    "#             train_acc.append(acc)\n",
    " \n",
    "\n",
    "#     print(\"  Loss    : \", \"%.3f\" % (np.mean(train_meta_loss)), end='\\t')\n",
    "#     print(\"  Accuracy: \", \"%.3f %%\" % (np.mean(train_acc) * 100),end='\\t')\n",
    "#     # print(\"  crsEnt    : \", \"%.3f\" % (np.mean(train_meta_crsEnt)), end='\\t')\n",
    "#     # print(\"  cls    : \", \"%.3f\" % (np.mean(train_meta_cls)), end='\\t')\n",
    "#     # print(\"  sep    : \", \"%.3f\" % (np.mean(train_meta_sep)), end='\\t')\n",
    "#     # print(\"  l1    : \", \"%.3f\" % (np.mean(train_meta_l1)))\n",
    "\n",
    "# #     See the validation accuracy after each epoch.\n",
    "# #     Early stopping is welcomed to implement.\n",
    "#     val_acc = []\n",
    "#     for eval_step in tqdm(range(6)):\n",
    "#             # len(val_loader) // (eval_batches))):\n",
    "#       x, val_iter,label_lst = get_meta_batch(\n",
    "#                 eval_batches, k_shot, q_query, \n",
    "#                 val_loader, val_iter)\n",
    "#             # We update three inner steps when testing.\n",
    "#       _, acc = MetaAlgorithm(meta_model, optimizer, x, \n",
    "#                     n_way, k_shot, q_query, \n",
    "#                     loss_fn,label_lst, \n",
    "#                     epoch,\n",
    "#                     eval_step,\n",
    "#                     inner_train_step=inner_train_step, \n",
    "#                     train=train\n",
    "#                     )  \n",
    "#       val_acc.append(acc)\n",
    "#     print(\"  Validation accuracy: \", \"%.3f %%\" % (np.mean(val_acc) * 100))\n",
    "#   return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bBIVZwgP037z"
   },
   "outputs": [],
   "source": [
    "# def new_outer_test(meta_model,MetaAlgorithm_new,optimizer,loss_fn,test_loader,test_iter,test_batches,n_way,k_shot,q_query,inner_train_step=15,train=2):\n",
    "#   test_acc = []\n",
    "#   for test_step in tqdm(range(6)):\n",
    "#           # len(test_loader) // (test_batches))):\n",
    "#     x, test_iter,label_lst = get_meta_batch(\n",
    "#             test_batches, k_shot, q_query, \n",
    "#             test_loader, test_iter)\n",
    "#     # When testing, we update 3 inner-steps\n",
    "#     _, acc = MetaAlgorithm_new(meta_model, optimizer, x, \n",
    "#             n_way, k_shot, q_query, \n",
    "#                       loss_fn,\n",
    "#                       30,\n",
    "#                       test_step,\n",
    "#                       inner_train_step=inner_train_step, \n",
    "#                       train=train) \n",
    "#     test_acc.append(acc)\n",
    "#   print(\"  Testing accuracy: \", \"%.3f %%\" % (np.mean(test_acc) * 100))\n",
    "#   return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "qWf09J4q037z"
   },
   "outputs": [],
   "source": [
    "# def copyConv(model_name,meta_model):\n",
    "#   model_ori=torch.load(model_name)\n",
    "#   meta_model.conv1.data.copy_(model_ori.conv1.data)\n",
    "#   meta_model.conv2.data.copy_(model_ori.conv2.data)\n",
    "#   meta_model.conv3.data.copy_(model_ori.conv3.data)\n",
    "#   meta_model.conv4.data.copy_(model_ori.conv4.data)\n",
    "#   return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "u1QEvQih037z"
   },
   "outputs": [],
   "source": [
    "# def new_outer(n_way,k_shot,q_query,in_ch,meta_lr,max_epoch,train_loader,val_loader,test_loader,train_iter,val_iter,test_iter\n",
    "#             ,meta_batch_size,eval_batches,test_batches,prototype_shape,img_size,epsilon,prototype_activation_function,inner_train_step=15,train=False,model_ori_name=False):\n",
    "#   meta_model, optimizer, loss_fn = model_init_new(n_way,in_ch,meta_lr,prototype_shape,img_size,epsilon,prototype_activation_function)\n",
    "#   MetaAlgorithm_new = MetaAlgorithmGeneratorNew()    \n",
    "#   try:\n",
    "#     meta_model = copyConv(model_ori_name,meta_model)\n",
    "#   except:\n",
    "#     print(\"can't find original model's name, please check 'model_ori_name'\")\n",
    "#   meta_model=new_outer_train_val(max_epoch\n",
    "#                 ,meta_model\n",
    "#                 ,MetaAlgorithm_new\n",
    "#                 ,optimizer\n",
    "#                 ,loss_fn\n",
    "#                 ,train_loader,val_loader\n",
    "#                 ,train_iter,val_iter\n",
    "#                 ,meta_batch_size,eval_batches\n",
    "#                 ,n_way\n",
    "#                 ,k_shot\n",
    "#                 ,q_query\n",
    "#                 ,inner_train_step=inner_train_step\n",
    "#                 ,train=train)\n",
    "#   meta_model=new_outer_test(meta_model\n",
    "#               ,MetaAlgorithm_new\n",
    "#               ,optimizer\n",
    "#               ,loss_fn\n",
    "#               ,test_loader\n",
    "#               ,test_iter\n",
    "#               ,test_batches\n",
    "#               ,n_way\n",
    "#               ,k_shot\n",
    "#               ,q_query\n",
    "#               ,inner_train_step=inner_train_step\n",
    "#               ,train=train)\n",
    "  \n",
    "#   return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSAyc5IM037z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84AzGh090370",
    "outputId": "6abd0f7c-758f-46bf-b263-ccaa6a29472f"
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "n_way = 5\n",
    "k_shot =1\n",
    "q_query =1\n",
    "# inner_train_step = 1\n",
    "# inner_lr = 0.01\n",
    "meta_lr = 0.001\n",
    "meta_batch_size = 4\n",
    "max_epoch = 5\n",
    "eval_batches = test_batches = 4\n",
    "in_ch=3\n",
    "prototype_shape=(20,64,1,1)\n",
    "img_size=224\n",
    "epsilon=1e-4\n",
    "prototype_activation_function='log'\n",
    "train_data_path = '/home/featurize/data/train/'\n",
    "val_data_path = '/home/featurize/data/val/'\n",
    "test_data_path = '/home/featurize/data/test/'\n",
    "# vgg16_bn_f = vgg16_bn_features(pretrained=True)\n",
    "# print(vgg16_bn_f)\n",
    "train_set = MiniImageNet(train_data_path, k_shot, q_query)\n",
    "val_set = MiniImageNet(val_data_path,k_shot,q_query)\n",
    "test_set = MiniImageNet(test_data_path, k_shot, q_query)\n",
    "\n",
    "(train_loader, val_loader, test_loader), \\\n",
    "(train_iter, val_iter, test_iter) = dataloader_init(\n",
    "                    (train_set, val_set, test_set),n_way)\n",
    "\n",
    "#   \n",
    "# meta_model,optimizer=ori_outer(n_way,k_shot,q_query,in_ch,meta_lr,max_epoch,train_loader,val_loader,test_loader,train_iter,val_iter,test_iter\n",
    "#               ,meta_batch_size,eval_batches,test_batches,inner_train_step=10,train=False,model_ori_name=False,optimizer=False)\n",
    "# return meta_model\n",
    "  # meta_new = new_outer(n_way,k_shot,q_query,in_ch,meta_lr,max_epoch,train_loader,val_loader,test_loader,train_iter,val_iter,test_iter\n",
    "  #          ,meta_batch_size,eval_batches,test_batches,prototype_shape,img_size,epsilon,prototype_activation_function,inner_train_step=15,train=False,model_ori_name='/content/drive/MyDrive/origin_20way1shot')\n",
    "  # return meta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556,
     "referenced_widgets": [
      "88163fb3993448c0ad6d58fd2652a6b9",
      "5ce45b74ddc744b29ae81cc1bd9a4c71",
      "b54a509dad18428c84cbbc6ce5512b1a",
      "215c2fd488c84100849eb2c6a93f5897",
      "5dc16a6d1f5c44e4a26da3a3f41c6e85",
      "622a10b45ade4d658b29eb631d4a1d9f",
      "d259efac3d4d4e18b94bb944603ef549",
      "4b6fd2c879924c8db44c3dead37c9aec"
     ]
    },
    "id": "ok8bYGXrlSe6",
    "outputId": "971fff92-01a1-412c-d3f4-7475da3c334e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't find original model's name, please check 'model_ori_name'\n",
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m5/\u001b[93m5\u001b[0m\u001b[0m   \u001b[99m00:01:24<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.06it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54623021decf4c2ab427c7e28821fa27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss    :  3.613\t  Accuracy:  54.000 %\n",
      "  Precision:  40.583 %\t  Recall:  54.000 %\t  F1:  44.167 %\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m6/\u001b[93m6\u001b[0m\u001b[0m   \u001b[99m00:02:16<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.04it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5167faffc0f43f9ad471f5f0fa7ad44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation accuracy:  55.833 %\n",
      "  Validation precision:  41.833 %\n",
      "  Validation recall:  55.833 %\n",
      "  Validation f1:  45.806 %\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m5/\u001b[93m5\u001b[0m\u001b[0m   \u001b[99m00:01:12<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.07it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905204500fc04897ab6060480e3c1afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss    :  6.950\t  Accuracy:  50.000 %\n",
      "  Precision:  35.867 %\t  Recall:  50.000 %\t  F1:  39.800 %\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m6/\u001b[93m6\u001b[0m\u001b[0m   \u001b[99m00:02:35<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.04it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208ec024f3584633ac789e31b7aef65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation accuracy:  49.167 %\n",
      "  Validation precision:  34.444 %\n",
      "  Validation recall:  49.167 %\n",
      "  Validation f1:  38.556 %\n",
      "Epoch 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m5/\u001b[93m5\u001b[0m\u001b[0m   \u001b[99m00:00:53<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.09it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806265ef44544c8587d3ac2e7a1a219e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss    :  9.547\t  Accuracy:  44.000 %\n",
      "  Precision:  28.483 %\t  Recall:  44.000 %\t  F1:  32.667 %\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m6/\u001b[93m6\u001b[0m\u001b[0m   \u001b[99m00:02:33<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.04it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a78cbdaacbb48179b8e54d97af0dbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation accuracy:  33.333 %\n",
      "  Validation precision:  17.972 %\n",
      "  Validation recall:  33.333 %\n",
      "  Validation f1:  21.028 %\n",
      "Epoch 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m5/\u001b[93m5\u001b[0m\u001b[0m   \u001b[99m00:01:30<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.06it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f0e6416481493e8a44575d519e1c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss    :  28.464\t  Accuracy:  27.000 %\n",
      "  Precision:  10.317 %\t  Recall:  27.000 %\t  F1:  13.667 %\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m6/\u001b[93m6\u001b[0m\u001b[0m   \u001b[99m00:02:31<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.04it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e992b29bf28546eeb67cc895fd7c4c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation accuracy:  23.333 %\n",
      "  Validation precision:  7.194 %\n",
      "  Validation recall:  23.333 %\n",
      "  Validation f1:  10.083 %\n",
      "Epoch 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m5/\u001b[93m5\u001b[0m\u001b[0m   \u001b[99m00:01:11<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.07it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91a9d7d2afe4d1cbeb914714e5ada03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss    :  42.517\t  Accuracy:  25.000 %\n",
      "  Precision:  9.050 %\t  Recall:  25.000 %\t  F1:  12.000 %\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m6/\u001b[93m6\u001b[0m\u001b[0m   \u001b[99m00:02:38<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.04it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69740bc4f19041569c7683efcbc778ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation accuracy:  25.833 %\n",
      "  Validation precision:  10.347 %\n",
      "  Validation recall:  25.833 %\n",
      "  Validation f1:  13.389 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
       "  \u001b[99m6/\u001b[93m6\u001b[0m\u001b[0m   \u001b[99m00:01:31<\u001b[93m00:00:00\u001b[0m\u001b[0m  \u001b[99m0.07it/s\u001b[0m                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3cd38664f549998e6a71a526c7dbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing accuracy:  25.833 %\n",
      "  Testing precision:  10.361 %\n",
      "  Testing recall:  25.833 %\n",
      "  Testing f1:  13.611 %\n"
     ]
    }
   ],
   "source": [
    "meta_model,optimizer=ori_outer(n_way,k_shot,q_query,in_ch,meta_lr,max_epoch,train_loader,val_loader,test_loader,train_iter,val_iter,test_iter\n",
    "              ,meta_batch_size,eval_batches,test_batches,inner_train_step=5,train=False,model_ori_name=False,optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "f2y3LLUv2NRu"
   },
   "outputs": [],
   "source": [
    "# meta_model,_,loss_fn=model_init(n_way,in_ch,meta_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "a97BjM16_e1V"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27285/2658039062.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "torch.load('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PRHdlJu6d1i"
   },
   "outputs": [],
   "source": [
    "# print(meta_model.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgC-dazS0370"
   },
   "outputs": [],
   "source": [
    "# x1=OrderedDict(meta_model.named_parameters())\n",
    "# print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObeUXe4X0371"
   },
   "outputs": [],
   "source": [
    "# torch.save(meta_model,'best3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DGRblEsiYN7"
   },
   "outputs": [],
   "source": [
    "# torch.save(meta_model,'origin_20way1shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zSoy-lGrmrv"
   },
   "outputs": [],
   "source": [
    "meta_model, optimizer, loss_fn = model_init(n_way,in_ch,meta_lr)\n",
    "MetaAlgorithm = MetaAlgorithmGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zjQIMpirmt_"
   },
   "outputs": [],
   "source": [
    "x, train_iter = get_meta_batch(\n",
    "    meta_batch_size, k_shot, q_query, \n",
    "    train_loader, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASXDhEjRwQY5"
   },
   "outputs": [],
   "source": [
    "for meta_batch in x:\n",
    "      # support_set = meta_batch[: n_way * k_shot]  \n",
    "      # query_set = meta_batch[n_way * k_shot :]\n",
    "      print(type(meta_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFtlOhFlrmwL"
   },
   "outputs": [],
   "source": [
    "# task_loss, task_acc = [], []\n",
    "# # special_grad = OrderedDict()  # Added for variants!\n",
    "# #         coefs = {\n",
    "# #             'crs_ent': 1,\n",
    "# #             'clst': 0.8,\n",
    "# #             'sep': -0.08,\n",
    "# #             'l1': 1e-4,\n",
    "# #             }\n",
    "# for meta_batch in x:\n",
    "#   support_set = meta_batch[: n_way * k_shot]  \n",
    "#   query_set = meta_batch[n_way * k_shot :]    \n",
    "  \n",
    "#   fast_weights = OrderedDict(meta_model.named_parameters())\n",
    "  \n",
    "#   ### ---------- INNER TRAIN LOOP ---------- ###\n",
    "#   # for inner_step in range(inner_train_step): \n",
    "#   train_label = create_label(n_way, k_shot).to(device)\n",
    "#   logits = meta_model.functional_forward(support_set, fast_weights)\n",
    "#   loss = loss_fn(logits, train_label)\n",
    "#   task_loss.append(loss)\n",
    "#   task_acc.append(calculate_accuracy(logits, train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_FWurl5rmyN"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,\n",
    "                # The \"batch_size\" here is not \\\n",
    "                #    the meta batch size, but  \\\n",
    "                #    how many different        \\\n",
    "                #    characters in a task,     \\\n",
    "                #    i.e. the \"n_way\" in       \\\n",
    "                #    few-shot classification.\n",
    "                batch_size=n_way,\n",
    "                num_workers=4,\n",
    "                shuffle=True,\n",
    "                drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbIAQeFRrm0N"
   },
   "outputs": [],
   "source": [
    "for i, (image, label) in enumerate(train_loader):\n",
    "  print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4nNiLNq4jBG"
   },
   "outputs": [],
   "source": [
    "class MiniImageNet(Dataset):\n",
    "  def __init__(self, data_dir,k_way,q_query):\n",
    "    \"\"\"Dataset class representing miniImageNet dataset\n",
    "\n",
    "    # Arguments:\n",
    "        subset: Whether the dataset represents the background or evaluation set\n",
    "    \"\"\"\n",
    "    self.file_list = [f for f in glob.glob(\n",
    "        data_dir+\"*\", \n",
    "        recursive=True)]\n",
    "    self.n = k_way + q_query        \n",
    "\n",
    "    normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                              std=(0.229, 0.224, 0.225))\n",
    "    # Setup transforms\n",
    "    self.transform = transforms.Compose([\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = np.arange(600)\n",
    "\n",
    "    # For random sampling the characters we want.\n",
    "    np.random.shuffle(sample) \n",
    "    img_path = self.file_list[idx]\n",
    "    img_list = [f for f in glob.glob(\n",
    "        img_path + \"**/*.jpg\", recursive=True)]\n",
    "    img_list.sort()\n",
    "    imgs = [self.transform(\n",
    "        Image.open(img_file)) \n",
    "        for img_file in img_list]\n",
    "    # `k_way + q_query` examples for each character\n",
    "    imgs = torch.stack(imgs)[sample[:self.n]] \n",
    "    lst = [img_path.split('/')[-1]]+[imgs]\n",
    "    return lst\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLwhFYaD4fWo"
   },
   "outputs": [],
   "source": [
    "train_data_path = '/content/miniImageNet/train/'\n",
    "val_data_path = '/content/miniImageNet/val/'\n",
    "test_data_path = '/content/miniImageNet/test'\n",
    "# vgg16_bn_f = vgg16_bn_features(pretrained=True)\n",
    "# print(vgg16_bn_f)\n",
    "train_set = MiniImageNet(train_data_path, k_shot, q_query)\n",
    "val_set = MiniImageNet(val_data_path,k_shot,q_query)\n",
    "test_set = MiniImageNet(test_data_path, k_shot, q_query)\n",
    "\n",
    "(train_loader, val_loader, test_loader), \\\n",
    "(train_iter, val_iter, test_iter) = dataloader_init(\n",
    "                    (train_set, val_set, test_set),n_way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPF4PWsW8mem"
   },
   "outputs": [],
   "source": [
    "def get_meta_batch(meta_batch_size,\n",
    "                   k_shot, q_query, \n",
    "                   data_loader, iterator):\n",
    "  data = []\n",
    "  label_lst =[]\n",
    "  for _ in range(meta_batch_size):\n",
    "    try:\n",
    "      # a \"task_data\" tensor is representing \\\n",
    "      #     the data of a task, with size of \\\n",
    "      #     [n_way, k_shot+q_query, 1, 28, 28]\n",
    "      task_data = iterator.next()  \n",
    "    except StopIteration:\n",
    "      iterator = iter(data_loader)\n",
    "      task_data = iterator.next()\n",
    "    label = task_data[0]\n",
    "    label_lst.append(label)\n",
    "    task_data = task_data[1]\n",
    "    train_data = (task_data[:, :k_shot]\n",
    "                  .reshape(-1, 3, 224, 224))\n",
    "    val_data = (task_data[:, k_shot:]\n",
    "                .reshape(-1, 3,224, 224))\n",
    "    task_data = torch.cat(\n",
    "        (train_data, val_data), 0)\n",
    "    data.append(task_data)\n",
    "  return torch.stack(data).to(device), iterator,label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebQVl7Gz8vIy"
   },
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for _ in range(meta_batch_size):\n",
    "#     try:\n",
    "#       # a \"task_data\" tensor is representing \\\n",
    "#       #     the data of a task, with size of \\\n",
    "#       #     [n_way, k_shot+q_query, 1, 28, 28]\n",
    "#       task_data = train_iter.next()  \n",
    "#     except StopIteration:\n",
    "#       iterator = iter(train_loader)\n",
    "#       task_data = iterator.next()\n",
    "    \n",
    "#     label = task_data[0]\n",
    "#     task_data= task_data[1]\n",
    "#     train_data = (task_data[:, :k_shot]\n",
    "#                   .reshape(-1, 3, 224, 224))\n",
    "#     # train_data = (task_data[:,1 :k_shot+1]\n",
    "#     #               .reshape(-1, 3, 224, 224))\n",
    "#     # print(train_data.shape())\n",
    "#     # print(train_data.shape())\n",
    "#     val_data = (task_data[:, k_shot:]\n",
    "#                 .reshape(-1, 3,224, 224))\n",
    "#     task_data = torch.cat(\n",
    "#         (train_data, val_data), 0)\n",
    "#     # data.append(task_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4i6s3QX8Z2k"
   },
   "outputs": [],
   "source": [
    "x, train_iter,label_lst = get_meta_batch(\n",
    "          meta_batch_size, k_shot, q_query, \n",
    "          train_loader, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBWp03tVCw9T"
   },
   "outputs": [],
   "source": [
    "np.array(label_lst).shape,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFHDODNbC60B"
   },
   "outputs": [],
   "source": [
    "label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4H6Ljl1JJDf5"
   },
   "outputs": [],
   "source": [
    "label=label_lst[0]\n",
    "list(label[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxbWeLC4JkW4"
   },
   "outputs": [],
   "source": [
    "def labeltoindex(label):\n",
    "  with open('/content/imagenet_index_class.json', 'r', encoding='utf8')as fp:\n",
    "    labels_all = json.load(fp)\n",
    "  index = labels_all[label]\n",
    "  return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5ngXRR0IxYW"
   },
   "outputs": [],
   "source": [
    "def find_label(label_lst,k_shot):\n",
    "  \n",
    "  labels = list(label_lst)\n",
    "  index_lst=[]\n",
    "  for label in labels:\n",
    "    index = labeltoindex(label)\n",
    "    index_lst=index_lst+[index]*k_shot\n",
    "    # index_lst.append(labeltoindex(label))\n",
    "  return torch.tensor(index_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvWtVjBbS9n8"
   },
   "outputs": [],
   "source": [
    "val_label=find_label(label,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCa4ifZcKf2W"
   },
   "outputs": [],
   "source": [
    "# print(find_label(label,1,1),type(find_label(label,1,1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ruh3E_8wDrcD"
   },
   "outputs": [],
   "source": [
    "train_label = create_label(n_way, 15).to(device)\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFi9qiiM2vD8"
   },
   "outputs": [],
   "source": [
    "res_lst=[]\n",
    "for meta_batch in x:\n",
    "      print(meta_batch.shape)\n",
    "      # vgg16_conv(meta_batch)\n",
    "      # support_set = meta_batch[: n_way * k_shot] \n",
    "      query_set = meta_batch[n_way * k_shot :] \n",
    "      # img = support_set[1:2,:,:,:]\n",
    "      # print(img.shape)\n",
    "      res= vgg16_conv(query_set)\n",
    "      # res_lst.append(res)\n",
    "      break\n",
    "      # query_set = meta_batch[n_way * k_shot :]    \n",
    "      # res=vgg16_conv(support_set)\n",
    "      # fast_weights = OrderedDict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uULwKEPOQGWH"
   },
   "outputs": [],
   "source": [
    "res_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aAITVG_PFfV"
   },
   "outputs": [],
   "source": [
    "support_set.shape\n",
    "for img in support_set:\n",
    "  print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRElmq3v5VQ3"
   },
   "outputs": [],
   "source": [
    "for i, t in enumerate(train_loader):\n",
    "  print(type(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCX-XhSyEx5M"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flT--HkUSQEO"
   },
   "outputs": [],
   "source": [
    "torch.argmax(res,-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7NSGcIKSn8q"
   },
   "outputs": [],
   "source": [
    "calculate_accuracy(res,val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwIO6jR6Z_PA"
   },
   "outputs": [],
   "source": [
    "loss_fn(res,val_label.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4dAARBaE54M"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, val_label):\n",
    "  \"\"\" utility function for accuracy calculation \"\"\"\n",
    "  torch.argmax(logits,)\n",
    "  acc = np.asarray([(\n",
    "      torch.argmax(logits, -1).cpu().numpy() == val_label.cpu().numpy())]\n",
    "      ).mean() \n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D00dcsHoFWxK"
   },
   "outputs": [],
   "source": [
    "vgg16_conv = Vgg16Conv()\n",
    "vgg16_conv.eval()\n",
    "vgg16_conv.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMV0JT5jOY9z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MAML_VGG16_miniImageNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "215c2fd488c84100849eb2c6a93f5897": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b6fd2c879924c8db44c3dead37c9aec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5ce45b74ddc744b29ae81cc1bd9a4c71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dc16a6d1f5c44e4a26da3a3f41c6e85",
      "placeholder": "​",
      "style": "IPY_MODEL_622a10b45ade4d658b29eb631d4a1d9f",
      "value": "  0.0%"
     }
    },
    "5dc16a6d1f5c44e4a26da3a3f41c6e85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "622a10b45ade4d658b29eb631d4a1d9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88163fb3993448c0ad6d58fd2652a6b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ce45b74ddc744b29ae81cc1bd9a4c71",
       "IPY_MODEL_b54a509dad18428c84cbbc6ce5512b1a"
      ],
      "layout": "IPY_MODEL_215c2fd488c84100849eb2c6a93f5897"
     }
    },
    "b54a509dad18428c84cbbc6ce5512b1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d259efac3d4d4e18b94bb944603ef549",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4b6fd2c879924c8db44c3dead37c9aec",
      "value": 0
     }
    },
    "d259efac3d4d4e18b94bb944603ef549": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
